# Bright Run LoRA Training Data Platform - User Journey Document
**Version:** 1.0  
**Date:** 10-26-2025  
**Category:** LoRA Fine-Tuning Training Data Platform User Journey
**Product Abbreviation:** train

**Source References:**
- Seed Story: `pmc/product/00-train-seed-story.md`
- Overview Document: `pmc/product/01-train-overview.md`
- User Stories: `pmc/product/02-train-user-stories.md`

---

## Executive Summary

### Product Vision Alignment

The Bright Run LoRA Training Data Platform transforms manual, console-based conversation generation into an intuitive UI-driven workflow that empowers non-technical business experts to create high-quality training datasets. This user journey document maps the complete experience from initial discovery through final data export, ensuring 95%+ time savings while maintaining rigorous quality standards.

**Core Value Delivered:**
- **95% Time Reduction**: Generate 90-100 conversations in hours instead of weeks
- **Zero Technical Expertise Required**: Smart 10th grader with AI basics can succeed
- **Complete Quality Control**: Human-in-the-loop approval ensures brand safety
- **Professional Output**: LoRA-ready training data for immediate fine-tuning

### Key User Personas Overview

#### Primary Persona: Sarah - Small Business Owner & Domain Expert
- **Background**: Financial advisor with 15 years experience, limited technical skills
- **Goals**: Create AI assistant that sounds like her and reflects her expertise
- **Pain Points**: Cannot afford AI engineers, frustrated by generic AI responses
- **Success Criteria**: Generate complete training dataset in one afternoon, approve 80%+ conversations

#### Secondary Persona: Michael - Content Manager
- **Background**: Knowledge manager responsible for IP curation and quality
- **Goals**: Organize conversations by business dimensions, ensure balanced coverage
- **Pain Points**: Manual tracking in spreadsheets, no visibility into generation progress
- **Success Criteria**: Review 20-30 conversations per hour, complete audit trail

#### Influencer Persona: Dr. Chen - CTO & Technical Leader
- **Background**: Technical decision maker evaluating custom AI platforms
- **Goals**: Balance business accessibility with technical rigor and scalability
- **Pain Points**: Need production-ready architecture, not prototypes
- **Success Criteria**: Normalized database scaling to thousands, 95%+ generation success rate

### Journey Scope and Boundaries

**In Scope:**
- Complete workflow from setup through LoRA-ready export
- Real-time progress tracking and status visibility
- Multi-dimensional filtering and organization
- Human approval workflow with quality validation
- Three-tier conversation architecture (Template/Scenario/Edge Case)
- Cost transparency and control

**Out of Scope:**
- User authentication setup (pre-existing from document categorization)
- Document upload and processing (Stage 1, already complete)
- Chunk extraction and dimensions (Stage 2, already complete)
- Advanced analytics and A/B testing (future enhancement)
- Multi-user collaboration features (future enhancement)

### Success Definition

**Quantitative Metrics:**
- 95%+ time savings vs. manual generation
- 80%+ approval rate on first generation
- Sub-2-hour completion for first 100 conversations
- 95%+ generation success rate
- <$2.00 cost per conversation

**Qualitative Success:**
- Users feel confident and in control
- Non-technical users complete independently
- Brand voice authenticity maintained
- Training data ready for immediate use
- Users recommend to peers

### Value Progression Story for Proof-of-Concept

The user journey progresses through six stages, each delivering increasing value:

1. **Discovery (15 min)**: User understands system, feels confident starting
2. **Setup (30 min)**: Conversation slots configured, cost/time clear
3. **Generation (60 min)**: Batch processing automated, progress visible
4. **Review (90 min)**: Quality control efficient, brand voice verified
5. **Validation (30 min)**: Coverage balanced, final approval confident
6. **Export (5 min)**: LoRA-ready data downloaded, integration path clear

**Total Time Investment**: 3-5 hours
**Value Delivered**: 90-100 approved, LoRA-ready training conversations
**Alternative (Manual)**: 2-3 weeks of repetitive console work

---

## User Persona Definitions

### Sarah - Small Business Owner & Domain Expert

**Role and Responsibilities:**
- Financial advisor with fiduciary responsibility
- Makes final decisions on brand voice and expertise representation
- Budget owner for AI training initiatives
- Ultimate quality gatekeeper

**Technical Proficiency Level:**
- Comfortable with: Email, Excel, Salesforce, online banking
- Unfamiliar with: APIs, JSON, prompts, fine-tuning, technical jargon
- AI Knowledge: Understands ChatGPT basics, wants personalized version
- Learning Preference: Video tutorials, clear examples, step-by-step guidance

**Goals and Motivations:**
- Create AI that sounds like her and reflects 15 years of expertise
- Launch personalized AI assistant within months (not years)
- Control budget and avoid unexpected costs
- Ensure AI never gives advice that contradicts her philosophy
- Maintain brand reputation and client trust

**Pain Points and Frustrations:**
- Cannot afford $200k+ for AI engineering team
- Generic AI doesn't understand her specific client situations
- Frustrated by technical tools requiring programming knowledge
- Anxiety about AI saying something inappropriate or wrong
- No time for weeks of manual work

**Success Criteria:**
- Generate 100 conversations in one afternoon
- Understand every step without technical jargon
- Review and approve each conversation before training
- See clear cost estimate before starting
- Export ready-to-use training file

**AI Knowledge and Learning:**
- Basic: Knows ChatGPT can answer questions
- Intermediate: Understands fine-tuning makes AI more specialized
- Needs: Simple explanations of why each step matters
- Prefers: "This is like..." analogies to familiar concepts

### Michael - Content Manager & Knowledge Steward

**Role and Responsibilities:**
- Manages company intellectual property and training content
- Daily user of conversation generation workflow
- Quality assurance and coverage analysis
- Exports training data for technical team

**Technical Proficiency Level:**
- Comfortable with: Databases, spreadsheets, analytics tools, content management
- Familiar with: Basic AI concepts, data quality metrics
- AI Knowledge: Understands training data quality impacts AI performance
- Learning Preference: Quick reference guides, keyboard shortcuts, bulk actions

**Goals and Motivations:**
- Ensure balanced representation across customer personas and emotions
- Maintain high quality standards (80%+ approval rate)
- Complete dataset review in reasonable time (4 hours for 100 conversations)
- Track coverage gaps and fill them systematically
- Provide clean, documented data to technical team

**Pain Points and Frustrations:**
- Manual tracking in spreadsheets is error-prone
- No visibility into which conversations generated, failed, pending
- Cannot filter by business dimensions (persona, emotion, topic)
- Repetitive clicking for bulk actions
- No audit trail for compliance reporting

**Success Criteria:**
- Review 20-30 conversations per hour
- Filter to specific persona/emotion combinations
- Approve/reject in bulk efficiently
- See coverage visualization showing gaps
- Export with complete metadata and quality statistics

### Dr. Chen - CTO & Technical Leader

**Role and Responsibilities:**
- Evaluates platforms for custom AI development
- Makes technical architecture decisions
- Ensures scalability and production-readiness
- Compliance and governance oversight

**Technical Proficiency Level:**
- Highly technical: Deep LLM, fine-tuning, and AI architecture knowledge
- Evaluates: Database design, API architecture, quality frameworks
- AI Knowledge: Expert level, understands training data trade-offs
- Learning Preference: Technical documentation, architecture diagrams, API reference

**Goals and Motivations:**
- Select platforms balancing business accessibility with technical rigor
- Ensure system scales to thousands of conversations
- Maintain complete audit trail for compliance
- Integration with standard LoRA pipelines
- Template versioning and A/B testing capability

**Pain Points and Frustrations:**
- Tools that sacrifice technical sophistication for simplicity
- Poor database design causing performance degradation
- Missing audit trails creating compliance risk
- Vendor lock-in with proprietary formats
- No visibility into API costs and rate limits

**Success Criteria:**
- Normalized database schema with indexes
- Query performance <500ms with 1000+ conversations
- 95%+ generation success rate with retry logic
- Open JSON format integrating with standard pipelines
- Complete API logs and audit trail

---

## 1. Discovery & Project Initialization

### 1.1 Platform Access & Navigation

**UJ1.1.1: Initial Platform Discovery**
* Description: User logs into Bright Run platform and discovers conversation generation module after completing document categorization and chunk extraction
* Impact Weighting: Adoption Influence / Time-to-Value
* Priority: High
* User Stories: IS3.6.0, US10.1.1
* Tasks: [To be populated]
* User Journey Acceptance Criteria:
  - GIVEN: User has completed document categorization and chunk extraction in previous stages
  - WHEN: User navigates to main dashboard or sees "Next Step" prompt
  - THEN: Clear call-to-action displays: "Generate Training Conversations" with estimated time (3-5 hours)
  - AND: Tooltip explains: "Create 90-100 conversations that train AI to sound like you"
  - AND: Status indicator shows: "Documents: ✓ Complete | Chunks: ✓ Complete | Conversations: Ready to Start"

Technical Notes: Navigation uses existing app router structure, seamless transition from chunks view
Data Requirements: Document and chunk completion status from previous stages
Error Scenarios: If documents or chunks not complete, show "Complete previous steps first" with links
Performance Criteria: Dashboard loads in <2 seconds
User Experience Notes: Visual progress indicator shows user is on Step 3 of 3, building confidence

**UJ1.1.2: Understanding the Workflow**
* Description: User views introductory overview explaining the conversation generation process in simple terms
* Impact Weighting: User Confidence / Ease of Use
* Priority: High
* User Stories: IS4.26.0
* Tasks: [To be populated]
* User Journey Acceptance Criteria:
  - GIVEN: User clicks "Generate Training Conversations" for first time
  - WHEN: Introduction modal opens
  - THEN: Simple explanation appears: "We'll create conversations between your AI and customers, based on your documents"
  - AND: Visual diagram shows: Your Documents → Conversation Examples → Trained AI
  - AND: Key benefits listed: "✓ 90-100 conversations in 3-5 hours | ✓ Review before training | ✓ Sounds like you"
  - AND: "Get Started" button prominent with "Skip Intro" option

Technical Notes: Modal only shows once per user, dismiss state saved
Data Requirements: User onboarding state
Error Scenarios: N/A
Performance Criteria: Modal renders instantly
User Experience Notes: Uses friendly, conversational tone; analogies to familiar concepts

### 1.2 Conversation Structure Understanding

**UJ1.2.1: Three-Tier Architecture Explanation**
* Description: User learns about Template, Scenario, and Edge Case conversation types in plain English
* Impact Weighting: Strategic Growth / User Understanding
* Priority: High
* User Stories: IS3.4.0, IS4.17.0
* Tasks: [To be populated]
* User Journey Acceptance Criteria:
  - GIVEN: User viewing dashboard for first time
  - WHEN: Hover over or click "What are tiers?" info icon
  - THEN: Explanation shows: "Think of this like learning - foundation first, then real situations, then challenging cases"
  - AND: Tier 1 (Template) described: "40 foundational conversations following emotional patterns like triumph or struggle"
  - AND: Tier 2 (Scenario) described: "35 real-world situations from your documents, like 'Client inherited $500k, wants advice'"
  - AND: Tier 3 (Edge Case) described: "15 challenging questions testing how AI handles unusual situations"
  - AND: Visual shows recommended distribution: 40/35/15 with "Why this balance?" explanation

Technical Notes: Contextual help system integrated throughout UI
Data Requirements: Default tier configuration, customizable
Error Scenarios: N/A
Performance Criteria: Instant tooltip display
User Experience Notes: Uses cooking analogy: "Master basics, try recipes, then improvise"

**UJ1.2.2: Emotional Arc Templates Preview**
* Description: User understands emotional patterns that structure Template tier conversations
* Impact Weighting: Ease of Use / Quality Understanding
* Priority: Medium
* User Stories: IS4.17.0, US8.1.2
* Tasks: [To be populated]
* User Journey Acceptance Criteria:
  - GIVEN: User exploring Template tier section
  - WHEN: Click "View Emotional Patterns" link
  - THEN: 5 patterns displayed: Triumph, Struggle-to-Success, Steady Confidence, Anxiety-to-Relief, Discovery
  - AND: Each pattern shows example: "Triumph: Client starts confident, achieves goal, ends celebrating success"
  - AND: User can preview sample conversation for each pattern
  - AND: Distribution adjustable: "How many of each pattern? Default: 8 each"

Technical Notes: Templates stored in database, user can customize distribution
Data Requirements: prompt_templates table with emotional arc definitions
Error Scenarios: If total doesn't equal 40, show validation error
Performance Criteria: Preview loads in <1 second
User Experience Notes: Examples use relatable scenarios relevant to user's industry

### 1.3 Cost & Time Transparency

**UJ1.3.1: Initial Cost Estimate Preview**
* Description: User sees upfront cost and time estimate before committing to generation
* Impact Weighting: Cost Transparency / User Confidence
* Priority: High
* User Stories: IS1.3.0, CS3
* Tasks: [To be populated]
* User Journey Acceptance Criteria:
  - GIVEN: User viewing conversation dashboard with 90 empty conversation slots
  - WHEN: Cost estimation displayed prominently
  - THEN: Estimate shows: "Generate 90 conversations: ~$8-12 | Time: 45-75 minutes"
  - AND: Breakdown visible: "Template tier: ~$0.08 each | Scenario: ~$0.12 each | Edge Case: ~$0.15 each"
  - AND: Comparison shown: "Manual alternative: 2-3 weeks of work"
  - AND: "Why these costs?" explanation: "Based on AI processing time and complexity"

Technical Notes: Cost calculation based on historical token usage per tier
Data Requirements: Average token counts per conversation type, current API pricing
Error Scenarios: If API pricing unavailable, show estimate range with warning
Performance Criteria: Calculation instant (pre-computed)
User Experience Notes: Cost framed as investment vs. alternative (hiring engineers, manual time)

**UJ1.3.2: Setting Budget Limits**
* Description: User optionally configures spending limit to prevent unexpected costs
* Impact Weighting: Cost Control / Risk Mitigation
* Priority: High
* User Stories: IS1.3.0, US13.1.1
* Tasks: [To be populated]
* User Journey Acceptance Criteria:
  - GIVEN: User reviewing cost estimate
  - WHEN: Click "Set Budget Limit" optional control
  - THEN: Input field appears: "Stop generation if cost exceeds: $____"
  - AND: Recommendations shown: "Typical: $15 for 100 conversations | Conservative: $10"
  - AND: Checkbox: "Pause (not stop) if limit reached - I'll decide whether to continue"
  - AND: Warning level configurable: "Alert me at 80% of limit"
  - AND: Settings saved to user profile for future generations

Technical Notes: Spending limit tracked in real-time during generation
Data Requirements: User preferences table storing budget settings
Error Scenarios: If limit set too low, warning: "This limit may not complete all conversations"
Performance Criteria: Settings save instantly
User Experience Notes: Defaults to "no limit" to avoid blocking first-time users; education over restriction

---

## 2. Content Ingestion & Automated Processing

### 2.1 Conversation Slot Configuration

**UJ2.1.1: Automatic Conversation Plan Generation**
* Description: System automatically proposes 90 conversation slots based on extracted chunks and dimensions
* Impact Weighting: Automation / Time Savings
* Priority: High
* User Stories: IS4.23.0, US10.2.1
* Tasks: [To be populated]
* User Journey Acceptance Criteria:
  - GIVEN: User has completed chunk extraction with 60 dimensions per chunk
  - WHEN: User initiates conversation generation workflow
  - THEN: System auto-generates 90 conversation plans distributed: 40 Template, 35 Scenario, 15 Edge Case
  - AND: Each plan pre-populated with: Persona (from chunks), Emotion (from emotional arc), Topic (from chunk content)
  - AND: User sees table: "90 conversations ready | 0 generated | 0 approved"
  - AND: Preview shows: "Anxious Investor | Fear | Portfolio Setup During Market Downturn"

Technical Notes: Plan generation algorithm selects diverse chunk combinations ensuring coverage
Data Requirements: Chunk dimensions (persona, topic, audience), emotional arc templates, tier distribution rules
Error Scenarios: If insufficient chunks (<30), show warning: "Add more source documents for better diversity"
Performance Criteria: Plan generation completes in <5 seconds
User Experience Notes: Users feel relieved system does hard work; editable if they want changes

**UJ2.1.2: Reviewing and Adjusting Conversation Plans**
* Description: User reviews auto-generated plans and optionally adjusts before generation
* Impact Weighting: User Control / Customization
* Priority: Medium
* User Stories: IS4.13.0, US6.1.1
* Tasks: [To be populated]
* User Journey Acceptance Criteria:
  - GIVEN: 90 conversation plans displayed in table
  - WHEN: User reviews table with columns: Tier, Persona, Emotion, Topic, Status
  - THEN: User can click any cell to edit (e.g., change emotion from "Fear" to "Anxiety")
  - AND: Validation ensures changes are valid (persona must match available values)
  - AND: "Reset to Auto-Generated" button available if user wants to undo changes
  - AND: Coverage visualization shows distribution: "6 personas × 5 emotions = 30 combinations covered"

Technical Notes: Inline editing with dropdown selectors for valid values
Data Requirements: Valid persona/emotion/topic lists from chunks and templates
Error Scenarios: If user creates duplicate combination, show info: "Similar conversation already exists at row 23"
Performance Criteria: Edit saves in <200ms
User Experience Notes: Most users accept defaults; power users appreciate customization

### 2.2 Batch Generation Initiation

**UJ2.2.1: Generate All Confirmation Dialog**
* Description: User confirms batch generation with final cost/time estimate and optional settings
* Impact Weighting: User Confidence / Cost Transparency
* Priority: High
* User Stories: IS1.3.0, IS1.4.0, US11.4.1
* Tasks: [To be populated]
* User Journey Acceptance Criteria:
  - GIVEN: User clicks "Generate All" button
  - WHEN: Confirmation dialog opens
  - THEN: Summary displays: "Generate 90 conversations | Estimated cost: $9.50 | Estimated time: 55 minutes"
  - AND: Breakdown shown: "Template (40): $3.20 | Scenario (35): $4.20 | Edge Case (15): $2.10"
  - AND: Warning if high cost: "Cost exceeds $15 - consider generating in smaller batches"
  - AND: Options: ☐ Email me when complete | ☐ Stop if spending limit exceeded
  - AND: "Proceed" button prominent, "Cancel" button equally accessible

Technical Notes: Cost/time calculated based on tier distribution and historical averages
Data Requirements: Token usage statistics, current API pricing, user email
Error Scenarios: If API key invalid, error: "Cannot connect to AI service - check settings"
Performance Criteria: Dialog opens instantly, calculations pre-computed
User Experience Notes: Transparency builds trust; users feel informed and in control

**UJ2.2.2: Batch Generation Starts with Clear Feedback**
* Description: User receives immediate confirmation that generation has started with progress initialization
* Impact Weighting: User Experience / Transparency
* Priority: High
* User Stories: IS2.1.1, US2.1.1
* Tasks: [To be populated]
* User Journey Acceptance Criteria:
  - GIVEN: User confirmed "Generate All"
  - WHEN: Batch generation starts
  - THEN: Success toast appears: "Batch generation started - 90 conversations queued"
  - AND: Dialog closes, dashboard shows progress section: "Generating: 0 of 90 complete (0%)"
  - AND: Current conversation displays: "Starting: Tier 1 Template conversations..."
  - AND: Estimated completion time shown: "Estimated finish: 3:45 PM (in 55 minutes)"
  - AND: Status updates every 2-5 seconds automatically

Technical Notes: Background job created, progress tracked in database
Data Requirements: conversations table with status field, generation start timestamp
Error Scenarios: If queue full, message: "Another generation in progress - wait or cancel it first"
Performance Criteria: Job creation <1 second, initial status display immediate
User Experience Notes: Immediate feedback reduces anxiety; users can navigate away knowing it's running

### 2.3 Real-Time Progress Monitoring

**UJ2.3.1: Multi-Level Progress Visualization**
* Description: User monitors generation progress with overall percentage, current conversation, and time remaining
* Impact Weighting: Transparency / User Experience
* Priority: High
* User Stories: IS2.1.0, EU1, US2.1.1
* Tasks: [To be populated]
* User Journey Acceptance Criteria:
  - GIVEN: Batch generation in progress
  - WHEN: User views dashboard
  - THEN: Progress bar shows: "42 of 90 conversations complete (47%)" with animated bar
  - AND: Current activity displays: "Generating: Confident Professional | Triumph | Successful Investment Strategy"
  - AND: Time remaining updates: "~28 minutes remaining (based on current rate)"
  - AND: Success/failure count shown: "40 succeeded | 2 failed (view errors)"
  - AND: Cost counter displays: "Spent: $4.40 of estimated $9.50"

Technical Notes: Polling every 2-5 seconds queries conversations table for status counts
Data Requirements: Real-time status counts aggregated by database
Error Scenarios: If connection lost, show: "Connection lost - progress continues, refresh to update"
Performance Criteria: Status query <100ms, UI updates smooth
User Experience Notes: Like watching food cook - users check periodically, feel informed

**UJ2.3.2: Background Processing Persistence**
* Description: User can close browser and generation continues, resuming progress view when returning
* Impact Weighting: User Experience / Reliability
* Priority: High
* User Stories: IS1.4.0, US1.3.2, US2.1.2
* Tasks: [To be populated]
* User Journey Acceptance Criteria:
  - GIVEN: Batch generation running
  - WHEN: User closes browser tab or navigates away
  - THEN: Generation continues on server without interruption
  - AND: When user returns to dashboard, banner shows: "Generation in Progress: 67 of 90 complete"
  - AND: Click banner to view detailed progress
  - AND: All progress metrics (count, time, cost) accurate and current
  - AND: If complete, success message: "Batch complete: 88 succeeded, 2 failed - Ready for review"

Technical Notes: Server-side background processing, status persisted in database
Data Requirements: Generation job status tracked independently of client connection
Error Scenarios: If server restart occurred, job resumes from last checkpoint
Performance Criteria: Job persistence survives server restart within 5 minutes
User Experience Notes: Users multitask freely; critical for long batches (60+ minutes)

---

## 3. Knowledge Exploration & Intelligent Organization

### 3.1 Conversation Discovery & Navigation

**UJ3.1.1: Dashboard Overview with Sortable Table**
* Description: User views all 90 conversations in organized table with key metadata and sort/filter capabilities
* Impact Weighting: Information Architecture / User Experience
* Priority: High
* User Stories: IS2.2.0, EU2, US6.1.1
* Tasks: [To be populated]
* User Journey Acceptance Criteria:
  - GIVEN: Generation complete, user viewing dashboard
  - WHEN: Conversation table loads
  - THEN: Table displays columns: ID, Persona, Emotion, Topic, Tier, Status, Quality Score, Generated Date
  - AND: Status badges color-coded: Green (Generated), Blue (Approved), Red (Failed), Gray (Not Generated)
  - AND: Quality scores show: Number (1-10) + color (Red <6, Yellow 6-7, Green 8-10)
  - AND: Click column header to sort ascending/descending with arrow indicator
  - AND: Pagination controls: "25 per page" with options for 50, 100

Technical Notes: Server-side pagination with indexed queries for performance
Data Requirements: conversations table with all metadata, indexed on frequently sorted fields
Error Scenarios: If no conversations, friendly empty state: "No conversations yet - click Generate All"
Performance Criteria: Table loads in <500ms for 100 conversations
User Experience Notes: Familiar spreadsheet-like interface; users scan quickly

**UJ3.1.2: Search and Quick Filtering**
* Description: User quickly finds specific conversations using text search across all fields
* Impact Weighting: Productivity / Ease of Use
* Priority: High
* User Stories: EU2, US6.1.1
* Tasks: [To be populated]
* User Journey Acceptance Criteria:
  - GIVEN: User looking for specific conversation
  - WHEN: Types in search bar: "portfolio setup"
  - THEN: Table filters to show only conversations matching "portfolio setup" in any field (Persona, Topic, Content)
  - AND: Match count displays: "Showing 8 of 90 conversations matching 'portfolio setup'"
  - AND: Search highlights matched terms in table cells
  - AND: Clear search button (X) resets to full table
  - AND: Search works across all pages (not just current page)

Technical Notes: Full-text search on persona, emotion, topic, and conversation content (turns)
Data Requirements: Database full-text search indexes or client-side filtering for small datasets
Error Scenarios: If no matches, message: "No conversations match 'xyz' - try different terms"
Performance Criteria: Search results appear in <200ms
User Experience Notes: Instant feedback as user types; forgiving search (finds partial matches)

### 3.2 Multi-Dimensional Filtering System

**UJ3.2.1: Persona and Emotion Filtering**
* Description: User filters conversations by persona and emotion to focus on specific customer types and emotional states
* Impact Weighting: Workflow Flexibility / Data Organization
* Priority: High
* User Stories: IS1.6.0, IS4.6.0, US3.1.1
* Tasks: [To be populated]
* User Journey Acceptance Criteria:
  - GIVEN: User wants to review all "Anxious Investor" conversations
  - WHEN: Opens "Persona" filter dropdown and selects "Anxious Investor"
  - THEN: Table updates showing only Anxious Investor conversations
  - AND: Filter badge appears: "Persona: Anxious Investor (✕)" with remove option
  - AND: Conversation count updates: "Showing 15 of 90 conversations"
  - AND: User adds emotion filter: "Fear"
  - AND: Table further narrows: "Showing 3 of 90 conversations" (Anxious Investor + Fear)
  - AND: Coverage hint: "3 conversations cover this combination - recommended: 5-7"

Technical Notes: Multi-select filters with AND logic, database WHERE clauses
Data Requirements: Indexed persona and emotion fields for fast filtering
Error Scenarios: If combination has 0 conversations, show: "No conversations match these filters"
Performance Criteria: Filter application <200ms
User Experience Notes: Visual feedback instant; users explore combinations interactively

**UJ3.2.2: Topic, Intent, and Tier Filtering**
* Description: User combines multiple dimension filters to drill down to very specific conversation subsets
* Impact Weighting: Power User Efficiency / Data Organization
* Priority: High
* User Stories: IS4.6.0, US3.1.1, US3.1.2
* Tasks: [To be populated]
* User Journey Acceptance Criteria:
  - GIVEN: User applied Persona and Emotion filters
  - WHEN: Adds Topic filter: "Portfolio Setup"
  - THEN: Table shows conversations matching all 3 filters
  - AND: Can further filter by Intent: "Seeking Advice" and Tier: "Scenario"
  - AND: Filter state persists in URL: `?persona=anxious-investor&emotion=fear&topic=portfolio-setup`
  - AND: User can bookmark or share URL to recreate exact filter state
  - AND: "Clear All Filters" button removes all filters at once
  - AND: Active filter count badge shows: "5 filters active"

Technical Notes: URL state management for shareability, database query combines all WHERE clauses
Data Requirements: All dimension fields indexed for multi-filter performance
Error Scenarios: If too many filters create empty result, suggest: "Try removing some filters"
Performance Criteria: Each additional filter adds <50ms to query time
User Experience Notes: Power users love precision; URL sharing helps collaboration

### 3.3 Coverage Analysis & Gap Identification

**UJ3.3.1: Visual Coverage Dashboard**
* Description: User views visual charts showing distribution of conversations across personas, emotions, and tiers
* Impact Weighting: Data Quality / Coverage Analysis
* Priority: Medium
* User Stories: IS1.6.0, IS4.7.0, US3.2.1
* Tasks: [To be populated]
* User Journey Acceptance Criteria:
  - GIVEN: User wants to see conversation balance
  - WHEN: Clicks "Coverage Analysis" button
  - THEN: Dashboard opens showing 3 charts:
    - Bar chart: Conversations per persona (6 bars, counts labeled)
    - Pie chart: Emotional arc distribution (5 slices: Triumph, Struggle, etc.)
    - Donut chart: Tier distribution (Template 40, Scenario 35, Edge Case 15)
  - AND: Heatmap shows persona × emotion matrix with conversation counts per cell
  - AND: Cells color-coded: Green (5+), Yellow (2-4), Red (0-1), showing gaps
  - AND: Export button downloads charts as PNG or coverage data as CSV

Technical Notes: Chart library (Recharts or D3.js) for visualization
Data Requirements: Aggregate queries counting conversations by dimension combinations
Error Scenarios: If no conversations generated, show: "Generate conversations first to see coverage"
Performance Criteria: Charts render in <1 second
User Experience Notes: Visual understanding faster than tables; gaps obvious

**UJ3.3.2: Gap Recommendations and Quick Actions**
* Description: User receives system recommendations for missing conversation combinations and can generate them directly
* Impact Weighting: Guidance / Quality Assurance
* Priority: Medium
* User Stories: IS1.6.0, IS4.7.0, US3.2.2
* Tasks: [To be populated]
* User Journey Acceptance Criteria:
  - GIVEN: Coverage analysis shows gaps
  - WHEN: User views recommendations panel
  - THEN: Top 5 missing combinations listed: "Confident Professional + Discovery + Estate Planning (0 conversations - Recommended: 3)"
  - AND: Each recommendation has "Generate" button
  - AND: Click generates conversation(s) for that specific combination
  - AND: Progress updates coverage visualization in real-time
  - AND: Recommendation updates or disappears when gap filled
  - AND: "Generate All Recommendations" button creates all missing combinations

Technical Notes: Recommendation algorithm identifies combinations with <3 conversations
Data Requirements: Matrix of all possible combinations, current counts per combination
Error Scenarios: If no gaps (perfect coverage), show: "Excellent coverage - all combinations represented"
Performance Criteria: Recommendations calculate in <500ms
User Experience Notes: Proactive guidance reduces cognitive load; users feel supported

---

## 4. Training Data Generation & Expert Customization

### 4.1 Conversation Content Preview

**UJ4.1.1: Formatted Turn-by-Turn Display**
* Description: User clicks conversation row to open formatted preview showing readable dialogue (not raw JSON)
* Impact Weighting: Quality Review / User Experience
* Priority: High
* User Stories: IS4.8.0, EU3, US4.1.1
* Tasks: [To be populated]
* User Journey Acceptance Criteria:
  - GIVEN: User wants to review specific conversation
  - WHEN: Clicks conversation row in table
  - THEN: Side panel slides open (or modal appears) showing formatted conversation
  - AND: Turn-by-turn display: "USER:" in blue, "ASSISTANT:" in green
  - AND: Readable typography: 16px font, 1.5 line height, proper spacing
  - AND: Scrollable for long conversations (16+ turns)
  - AND: Metadata panel shows: Persona, Emotion, Topic, Intent, Tone, Tier, Quality Score (8/10), Generated Date
  - AND: Close button (X) or ESC key to dismiss

Technical Notes: Conversation turns fetched via API, formatted client-side
Data Requirements: conversation_turns table joined to conversations
Error Scenarios: If turns missing, show: "Conversation content unavailable - regenerate"
Performance Criteria: Preview opens in <500ms
User Experience Notes: Reading experience like messaging app; natural and familiar

**UJ4.1.2: Preview Navigation Between Conversations**
* Description: User navigates between conversations using Previous/Next buttons without closing preview panel
* Impact Weighting: Productivity / Review Efficiency
* Priority: High
* User Stories: IS4.8.0, US4.1.2
* Tasks: [To be populated]
* User Journey Acceptance Criteria:
  - GIVEN: User has conversation preview open
  - WHEN: Clicks "Next" button or presses Right Arrow key
  - THEN: Preview loads next conversation in table order
  - AND: Metadata and conversation content update
  - AND: Position indicator shows: "Conversation 3 of 25" (respects current filters)
  - AND: Previous/Next buttons disabled at first/last conversation
  - AND: Navigation preserves filter state (only navigates within filtered results)
  - AND: Smooth transition animation between conversations

Technical Notes: Client-side navigation through filtered conversation list
Data Requirements: Current conversation list (filtered) from table state
Error Scenarios: If next conversation fails to load, show error and stay on current
Performance Criteria: Navigation <300ms per conversation
User Experience Notes: Efficient review workflow; users stay in "flow state"

### 4.2 Quality Assessment Understanding

**UJ4.2.1: Quality Score Breakdown**
* Description: User clicks quality score to see detailed validation criteria explaining the score
* Impact Weighting: Transparency / Learning
* Priority: Medium
* User Stories: IS4.10.0, US4.3.1, US4.3.2
* Tasks: [To be populated]
* User Journey Acceptance Criteria:
  - GIVEN: User sees quality score "7/10" on conversation
  - WHEN: Clicks score to view details
  - THEN: Breakdown modal opens showing:
    - Turn Count Score: 4/5 (12 turns - optimal: 8-16)
    - Length Score: 4/5 (Responses average 80 words - good range)
    - Structure Score: 5/5 (Valid JSON, proper formatting)
    - Confidence Score: 3/5 (Model confidence: medium)
    - Overall: 7/10 (Good quality - likely acceptable)
  - AND: Visual progress bars for each criterion
  - AND: Explanation: "Turn Count: 12 turns detected. Optimal range is 8-16. Score: 4/5."
  - AND: Recommendation: "Quality is good. Review content to ensure brand voice."

Technical Notes: Quality scoring algorithm runs post-generation, stored in quality_score field
Data Requirements: Quality validation results stored in conversation metadata or separate table
Error Scenarios: If quality data missing, show: "Quality score unavailable - regenerate"
Performance Criteria: Modal opens instantly (data pre-loaded)
User Experience Notes: Education builds user understanding of quality standards

**UJ4.2.2: Low-Quality Conversation Flagging**
* Description: User easily identifies low-quality conversations (<6 score) requiring attention
* Impact Weighting: Quality Assurance / Efficiency
* Priority: Medium
* User Stories: IS4.10.0, IS4.9.0, US4.3.1
* Tasks: [To be populated]
* User Journey Acceptance Criteria:
  - GIVEN: Batch generation complete
  - WHEN: User views dashboard
  - THEN: Low-quality conversations (<6) highlighted with red quality badge
  - AND: Warning icon (⚠️) next to score: "Review Recommended"
  - AND: Dashboard header shows: "5 conversations need review (quality <6)"
  - AND: Quick filter button: "Show Low Quality Only"
  - AND: Sort by quality (ascending) brings low-quality to top
  - AND: Hover tooltip explains: "Low quality - check for errors or regenerate"

Technical Notes: Quality threshold configurable (default: 6)
Data Requirements: Quality score indexed for fast filtering
Error Scenarios: If no low-quality conversations, show: "All conversations meet quality standards"
Performance Criteria: Flag highlighting instant, filter <200ms
User Experience Notes: Visual cues guide attention; users prioritize efficiently

### 4.3 Approval Workflow

**UJ4.3.1: Single Conversation Approval**
* Description: User approves or rejects individual conversation with optional notes documenting decision
* Impact Weighting: Quality Control / Business Value
* Priority: High
* User Stories: IS1.2.0, CS2, IS4.9.0, US4.2.1
* Tasks: [To be populated]
* User Journey Acceptance Criteria:
  - GIVEN: User reviewing conversation in preview panel
  - WHEN: Clicks "Approve" button
  - THEN: Status immediately updates to "Approved" with green checkmark badge
  - AND: Success toast: "Conversation approved"
  - AND: If "Next" clicked, moves to next conversation for efficient review
  - AND: Alternatively, clicking "Reject" updates status to "Rejected" with red X badge
  - AND: Optional notes field: "Why rejecting?" with 500 char limit
  - AND: Rejected conversations excluded from export but retained for analysis
  - AND: Audit trail records: reviewer_id, action (approve/reject), timestamp, notes

Technical Notes: PATCH /api/conversations/:id updates status, creates review log entry
Data Requirements: conversations table (status, approved_by, approved_at, reviewer_notes), review_logs table
Error Scenarios: If update fails, show error and revert UI optimistically updated state
Performance Criteria: Approval action <200ms
User Experience Notes: Instant feedback; users move quickly through reviews

**UJ4.3.2: Bulk Approval Actions**
* Description: User selects multiple conversations and approves/rejects them efficiently using checkboxes
* Impact Weighting: Operational Efficiency / Time Savings
* Priority: High
* User Stories: IS2.4.0, EU3, US4.2.2, US6.2.2
* Tasks: [To be populated]
* User Journey Acceptance Criteria:
  - GIVEN: User reviewed multiple conversations
  - WHEN: Checks boxes for 10 conversations and clicks "Approve Selected"
  - THEN: Confirmation dialog: "Approve 10 conversations?"
  - AND: Optional bulk notes field: "Add note to all (optional)"
  - AND: Click "Confirm" processes all 10 sequentially with progress indicator
  - AND: Success message: "10 conversations approved"
  - AND: If any fail, detailed feedback: "8 approved, 2 failed (view errors)"
  - AND: Table updates showing all approved conversations with green badges

Technical Notes: POST /api/conversations/bulk-action with conversation IDs and action
Data Requirements: Same as single approval, batch processing
Error Scenarios: Partial success handled gracefully; successful actions not rolled back
Performance Criteria: Bulk action processes 5-10 conversations per second
User Experience Notes: Power users appreciate efficiency; progress bar reduces anxiety

### 4.4 Conversation Regeneration

**UJ4.4.1: Single Conversation Regeneration**
* Description: User regenerates low-quality or rejected conversation to get better version
* Impact Weighting: Quality Control / Flexibility
* Priority: Medium
* User Stories: IS4.1.0, US1.1.1
* Tasks: [To be populated]
* User Journey Acceptance Criteria:
  - GIVEN: User viewing low-quality conversation (score: 5/10)
  - WHEN: Clicks "Regenerate" button in preview panel
  - THEN: Confirmation dialog: "Regenerate this conversation? Current version will be replaced. Cost: ~$0.12"
  - AND: Click "Proceed" starts regeneration, status changes to "Generating"
  - AND: Progress indicator shows: "Regenerating... ~30 seconds"
  - AND: When complete, preview updates with new conversation content
  - AND: Quality score recalculated and displayed
  - AND: Old version moved to version history (accessible if needed)

Technical Notes: New generation job created for single conversation, old version archived
Data Requirements: conversation_turns table allows multiple versions per conversation_id
Error Scenarios: If regeneration fails, original version preserved
Performance Criteria: Regeneration completes in 15-45 seconds
User Experience Notes: Users iterate toward perfection; version history provides safety net

---

## 5. Collaborative Quality Control & Final Validation

### 5.1 Approval Status Tracking

**UJ5.1.1: Approval Progress Visibility**
* Description: User sees clear progress toward 100% approval goal with visual indicators
* Impact Weighting: Progress Tracking / Motivation
* Priority: High
* User Stories: IS2.2.0, EU2
* Tasks: [To be populated]
* User Journey Acceptance Criteria:
  - GIVEN: User has reviewed some conversations
  - WHEN: Viewing dashboard header
  - THEN: Approval progress displays: "72 of 90 approved (80%)" with progress bar
  - AND: Breakdown shows: "72 Approved | 5 Rejected | 13 Need Review"
  - AND: Goal reminder: "Target: 80%+ approval rate for high-quality dataset"
  - AND: Visual progress bar fills green as approvals increase
  - AND: Milestone celebrations: "Great! 80% approved - ready for export"

Technical Notes: Real-time count from conversations table WHERE status='approved'
Data Requirements: Approval status counts aggregated
Error Scenarios: N/A
Performance Criteria: Count updates instantly as approvals change
User Experience Notes: Gamification motivates; users feel progress toward completion

**UJ5.1.2: Reviewer Activity Audit Trail**
* Description: User (especially compliance officer) views complete audit trail showing who reviewed what when
* Impact Weighting: Compliance / Accountability
* Priority: Medium
* User Stories: IS2.8.0, IS3.8.0, US9.3.2
* Tasks: [To be populated]
* User Journey Acceptance Criteria:
  - GIVEN: Compliance officer needs audit report
  - WHEN: Opens "Review Activity" page
  - THEN: Activity log displays all review events:
    - Sarah Johnson approved "Anxious Investor + Fear + Portfolio Setup" at 2:35 PM
    - Sarah Johnson rejected "Confident Professional + Triumph + Estate Planning" at 2:38 PM (Note: "Too generic")
  - AND: Filterable by: reviewer, date range, action (approve/reject)
  - AND: Export button downloads CSV: conversation_id, reviewer, action, timestamp, notes
  - AND: Statistics shown: "Sarah: 72 reviewed (68 approved, 4 rejected) | Avg review time: 2.5 min/conversation"

Technical Notes: review_logs table stores all approval/rejection events
Data Requirements: review_logs table with foreign keys to conversations and users
Error Scenarios: If no activity, show: "No reviews yet"
Performance Criteria: Log query <500ms for 1000+ entries
User Experience Notes: Transparency builds trust; compliance officers require this

### 5.2 Final Coverage Validation

**UJ5.2.1: Coverage Completeness Check**
* Description: User validates that all persona/emotion combinations are adequately represented before export
* Impact Weighting: Data Quality / Completeness
* Priority: Medium
* User Stories: IS1.6.0, IS4.7.0, US3.2.1
* Tasks: [To be populated]
* User Journey Acceptance Criteria:
  - GIVEN: User ready to export
  - WHEN: Clicks "Validate Coverage" button
  - THEN: Coverage report shows:
    - Persona coverage: 6/6 personas represented (✓)
    - Emotion coverage: 5/5 emotions represented (✓)
    - Tier coverage: Template (40) | Scenario (35) | Edge Case (15) (✓)
    - Combination coverage: 28 of 30 persona×emotion combinations covered (93%)
  - AND: Warning if gaps: "2 combinations missing: Confident Professional + Anxiety-to-Relief (0), Cautious Retiree + Discovery (1)"
  - AND: Recommendation: "Add 2 more conversations to achieve 100% coverage"
  - AND: Quick action: "Generate Missing Combinations"

Technical Notes: Validation algorithm checks all combinations against target distribution
Data Requirements: Approved conversations aggregated by dimensions
Error Scenarios: If <50% coverage, error: "Insufficient coverage - generate more conversations"
Performance Criteria: Validation completes in <1 second
User Experience Notes: Pre-export validation prevents incomplete datasets

**UJ5.2.2: Quality Distribution Validation**
* Description: User ensures quality scores are well-distributed before final export
* Impact Weighting: Quality Assurance / Risk Mitigation
* Priority: Medium
* User Stories: IS3.5.0, US4.3.1
* Tasks: [To be populated]
* User Journey Acceptance Criteria:
  - GIVEN: User validated coverage
  - WHEN: Views quality distribution report
  - THEN: Chart shows: "Quality Score Distribution: High (8-10): 52 | Medium (6-7): 18 | Low (<6): 2"
  - AND: Percentage: "72% high quality - Excellent!"
  - AND: Warning if skewed: "Only 40% high quality - consider regenerating low-quality conversations"
  - AND: Recommendation: "Reject or regenerate 2 low-quality conversations before export"
  - AND: Export warning appears if exporting <80% high-quality conversations

Technical Notes: Quality score distribution calculated from approved conversations
Data Requirements: Quality_score field aggregated
Error Scenarios: If too many low-quality, block export with: "Improve quality first"
Performance Criteria: Distribution calculation <500ms
User Experience Notes: Quality gate prevents poor training data; users appreciate guidance

### 5.3 Cost Summary and Budget Reconciliation

**UJ5.3.1: Final Cost Summary**
* Description: User views complete cost breakdown showing actual vs. estimated spend
* Impact Weighting: Cost Transparency / Reporting
* Priority: High
* User Stories: IS1.3.0, CS3, US13.2.1
* Tasks: [To be populated]
* User Journey Acceptance Criteria:
  - GIVEN: All generation complete
  - WHEN: User views cost summary section
  - THEN: Final cost displays: "Total Cost: $9.72 (Estimated: $9.50, +2% variance)"
  - AND: Breakdown by tier: "Template: $3.18 | Scenario: $4.28 | Edge Case: $2.26"
  - AND: Per-conversation average: "$0.108 per conversation (90 total)"
  - AND: Comparison: "20% lower than previous batch ($12.15)"
  - AND: Download button: "Export Cost Report (CSV)"

Technical Notes: Cost tracked per conversation during generation, aggregated for summary
Data Requirements: actual_cost_usd and estimated_cost_usd fields in conversations table
Error Scenarios: If cost data missing, estimate based on tokens
Performance Criteria: Summary calculation instant (pre-aggregated)
User Experience Notes: Transparency builds trust; users appreciate predictability

---

## 6. Synthetic Data Expansion & Value Amplification

### 6.1 Export Preparation

**UJ6.1.1: Pre-Export Filtering and Selection**
* Description: User filters conversations to approved-only and reviews final export list
* Impact Weighting: Quality Control / Export Accuracy
* Priority: High
* User Stories: IS1.5.0, IS4.11.0, US5.1.1
* Tasks: [To be populated]
* User Journey Acceptance Criteria:
  - GIVEN: User ready to export
  - WHEN: Clicks "Export" button
  - THEN: Export dialog opens with filters pre-set: "Status: Approved Only"
  - AND: Summary shows: "72 conversations selected for export"
  - AND: Breakdown visible: "Template: 38 | Scenario: 24 | Edge Case: 10"
  - AND: User can adjust filters if needed (export subset by persona, tier, etc.)
  - AND: Warning if <70 conversations: "Small dataset may limit training effectiveness"
  - AND: Preview button shows first 3 conversations in export format

Technical Notes: Export dialog pre-filters to approved status by default
Data Requirements: conversations WHERE status='approved'
Error Scenarios: If no approved conversations, error: "Approve conversations before export"
Performance Criteria: Dialog opens <500ms
User Experience Notes: Approved-only default prevents accidental export of rejected content

**UJ6.1.2: Format Selection and Preview**
* Description: User selects export format (OpenAI, Anthropic, generic JSON) and previews structure
* Impact Weighting: Integration / Use Case Support
* Priority: High
* User Stories: IS4.11.0, IS4.12.0, US5.1.1
* Tasks: [To be populated]
* User Journey Acceptance Criteria:
  - GIVEN: User selected conversations to export
  - WHEN: Views format selection dropdown
  - THEN: Options shown: "OpenAI Fine-Tuning Format | Anthropic Claude Format | Generic JSONL"
  - AND: Format description for each: "OpenAI: For use with OpenAI fine-tuning API"
  - AND: Preview pane shows sample conversation in selected format:
    ```json
    {"messages": [
      {"role": "user", "content": "I inherited $500k..."},
      {"role": "assistant", "content": "Congratulations! Let's discuss..."}
    ]}
    ```
  - AND: Metadata section preview shows: export_date, conversation_count, quality_stats

Technical Notes: Format conversion logic in exportService
Data Requirements: Conversation turns formatted per selected standard
Error Scenarios: If format not supported, default to generic JSON
Performance Criteria: Preview renders instantly (<200ms)
User Experience Notes: Preview builds confidence; users verify structure before download

### 6.2 Export Execution

**UJ6.2.1: Initiating Export and Download**
* Description: User confirms export and downloads LoRA-ready training data file
* Impact Weighting: Training Data Delivery / Integration
* Priority: High
* User Stories: IS1.5.0, IS4.11.0, US5.1.1
* Tasks: [To be populated]
* User Journey Acceptance Criteria:
  - GIVEN: User confirmed format and filters
  - WHEN: Clicks "Download" button
  - THEN: Export generation starts with progress indicator: "Generating export file..."
  - AND: File generates in 2-5 seconds for 72 conversations
  - AND: Browser downloads file: `training-data-2025-10-26-approved-72-conversations.json`
  - AND: Success toast: "Export complete: 72 conversations downloaded"
  - AND: File opens in text editor showing properly formatted JSON
  - AND: Metadata header includes: export_date, format_version, quality_stats, tier_distribution

Technical Notes: POST /api/export/conversations generates file server-side, returns download URL
Data Requirements: Approved conversations formatted, metadata compiled
Error Scenarios: If export fails, error: "Export failed - try again or contact support"
Performance Criteria: Export generation <5 seconds for 100 conversations
User Experience Notes: Immediate download gratifying; users feel completion

**UJ6.2.2: Export Confirmation and Next Steps**
* Description: User receives confirmation with export summary and guidance on next steps
* Impact Weighting: User Confidence / Onward Journey
* Priority: Medium
* User Stories: IS1.5.0, US5.1.1
* Tasks: [To be populated]
* User Journey Acceptance Criteria:
  - GIVEN: Export downloaded successfully
  - WHEN: User sees confirmation dialog
  - THEN: Summary displays:
    - "✓ Export Complete: 72 conversations"
    - "✓ Format: OpenAI Fine-Tuning"
    - "✓ Quality: 70% high quality (8-10 score)"
    - "✓ Coverage: 93% persona×emotion combinations"
    - "✓ File: training-data-2025-10-26-approved-72-conversations.json (1.2 MB)"
  - AND: Next steps guidance: "Next: Upload this file to your LoRA fine-tuning platform"
  - AND: Links to integration guides: "OpenAI Fine-Tuning Guide | RunPod Tutorial"
  - AND: Option: "Email me this summary and file link"

Technical Notes: Export log entry created in export_logs table
Data Requirements: Export metadata, file path, user email
Error Scenarios: N/A
Performance Criteria: Confirmation appears immediately after download
User Experience Notes: Closure and guidance; users feel successful and know what's next

### 6.3 Post-Export Actions

**UJ6.3.1: Export History Tracking**
* Description: User views history of all exports with metadata for audit and reproducibility
* Impact Weighting: Compliance / Reproducibility
* Priority: Low
* User Stories: IS2.8.0, US5.2.2, US9.3.3
* Tasks: [To be populated]
* User Journey Acceptance Criteria:
  - GIVEN: User exported multiple times
  - WHEN: Opens "Export History" page
  - THEN: Table shows all exports: Date, User, Format, Conversation Count, Filters Applied, Download Link
  - AND: Click export entry to see full details: filter state, quality stats, tier distribution
  - AND: "Re-run Export" button recreates exact same export with current data
  - AND: Download link remains active for 90 days
  - AND: Export to CSV for compliance reporting

Technical Notes: export_logs table with foreign keys to users and filter state (JSONB)
Data Requirements: All export events logged with complete metadata
Error Scenarios: If download link expired, message: "File expired - re-run export"
Performance Criteria: History page loads <500ms for 100+ exports
User Experience Notes: Audit trail for compliance; reproducibility for iterations

**UJ6.3.2: Conversation Dataset Iteration**
* Description: User understands they can generate additional conversations or iterate on rejected ones
* Impact Weighting: Continuous Improvement / Flexibility
* Priority: Low
* User Stories: IS4.2.0
* Tasks: [To be populated]
* User Journey Acceptance Criteria:
  - GIVEN: User exported initial dataset
  - WHEN: Views dashboard post-export
  - THEN: Guidance panel shows: "Dataset exported - You can continue improving!"
  - AND: Options displayed:
    - "Generate More Conversations" (add 20, 50, or custom count)
    - "Regenerate Rejected Conversations" (improve quality)
    - "Fill Coverage Gaps" (target missing combinations)
  - AND: Export again anytime with updated approved conversations
  - AND: Version tracking: "Dataset v1 (72 conversations) | v2 (90 conversations)"

Technical Notes: Conversations persist in database, can always add/regenerate/re-export
Data Requirements: Conversation state preserved, version metadata in export logs
Error Scenarios: N/A
Performance Criteria: N/A
User Experience Notes: Users understand this is iterative, not one-shot; reduces pressure

---

## Cross-Stage Integration

### User Journey Flow

**Complete Experience Progression:**
1. **Discovery (15 min)**: User logs in → sees conversation module → understands workflow → feels confident
2. **Setup (30 min)**: Automatically generated 90 plans → reviews/adjusts → sets budget → confirms generation
3. **Generation (60 min)**: Batch starts → monitors progress → navigates away → returns to completion
4. **Organization (30 min)**: Explores table → filters by dimensions → identifies low-quality → prioritizes review
5. **Review (90 min)**: Previews conversations → approves good ones → rejects poor ones → regenerates as needed
6. **Validation (30 min)**: Validates coverage → checks quality distribution → reviews cost → prepares export
7. **Export (5 min)**: Selects format → downloads file → receives confirmation → understands next steps

**Total Time: 3-5 hours**
**Alternative (Manual Console): 2-3 weeks**

### Value Amplification

**How Stages Compound User Value:**

**Stage 1 + 2**: Chunked documents provide rich context for conversation generation
**Stage 2 + 3**: 60 dimensions enable precise conversation planning (persona, emotion, topic auto-populated)
**Stage 3 + 4**: Filtering reveals conversations needing attention, focusing review time efficiently
**Stage 4 + 5**: Approval workflow ensures only high-quality conversations exported
**Stage 5 + 6**: Coverage validation prevents incomplete datasets, maximizing training effectiveness
**Stage 6 + Future**: Exported data ready for LoRA fine-tuning, creating personalized AI

**10-100x Multiplication**: 
- Expert creates 10 seed conversations (8 hours)
- System generates 90 synthetic variations (1 hour automated)
- 100 total conversations created in 9 hours vs. 80 hours manual (9x efficiency)
- Future: Seed 10 → Generate 1000 (100x multiplication)

### Development Efficiency

**How Sequence Optimizes Development:**

**Phase 1 (Weeks 1-3)**: Single conversation generation proves core technology
**Phase 2 (Weeks 4-6)**: Batch + progress tracking demonstrate scale
**Phase 3 (Weeks 7-8)**: Filtering + approval + export complete experience

**Parallel Development Possible:**
- Frontend (table, filters, preview) independent from backend generation
- Export formatting independent from generation engine
- Quality validation independent from approval UI

**Reusable Components:**
- Conversation table reused from chunk view
- Progress tracking pattern from chunk extraction
- API integration pattern from document processing

### Data Flow Between Stages

**From Stage 1 (Documents) to Stage 3 (Conversations):**
- Document categories inform conversation topics
- Statement of Belonging level guides emotional tone
- Business value ranking prioritizes which documents generate most conversations

**From Stage 2 (Chunks) to Stage 3 (Conversations):**
- Chunk dimensions auto-populate conversation metadata:
  - Summary → Topic
  - Audience Level → Complexity
  - Emotional Valence → Emotion
  - Content Category → Intent
- Chunk content provides scenario basis for Tier 2 (Scenario conversations)
- Expertise level determines conversation sophistication

**From Stage 3 to Future Training:**
- Approved conversations become LoRA training data
- Quality scores predict model performance
- Coverage metrics ensure balanced training
- Export format enables immediate fine-tuning

### Progressive Enhancement

**Foundation Elements Each Stage Establishes:**

**Stage 1 establishes**: Trust in platform, understanding of document value, confidence in AI processing
**Stage 2 establishes**: Comfort with semantic analysis, appreciation for dimensional organization, patience for processing time
**Stage 3 establishes**: Understanding of conversation quality, ability to assess brand voice, confidence in approval decisions

**Value Amplification:**
- Each stage validates previous stage value
- Each stage increases user investment and commitment
- Each stage delivers tangible output (documents categorized → chunks extracted → conversations generated)
- Each stage builds user expertise and confidence

### Rollback Scenarios

**How Users Return to Previous Stages:**

**From Conversations back to Chunks:**
- User realizes source content quality insufficient
- Returns to Stage 2 to review/regenerate chunks
- Conversation plans persist, regenerate with improved chunks

**From Export back to Review:**
- User discovers rejected conversations worth reconsidering
- Returns to approval workflow, reviews again
- Re-exports with updated approval set

**From Generation back to Setup:**
- User cancels batch mid-generation
- Returns to conversation plan configuration
- Adjusts distribution (40/35/15 → 50/30/10)
- Regenerates with new plan

**Safety Nets:**
- All stages non-destructive (can always regenerate)
- Version history preserves previous states
- Export history enables reproducing previous datasets
- Rejected conversations retained for analysis

---

## Acceptance Criteria Inventory

### Consolidated List with UJ References

#### Critical Priority (Must Have for MVP)

| UJ ID | Element | Priority | Effort | Risk | User Impact |
|-------|---------|----------|--------|------|-------------|
| UJ1.1.1 | Initial Platform Discovery | Critical | Low | Low | High |
| UJ1.1.2 | Understanding the Workflow | Critical | Low | Low | High |
| UJ1.2.1 | Three-Tier Architecture Explanation | Critical | Low | Low | High |
| UJ1.3.1 | Initial Cost Estimate Preview | Critical | Medium | Medium | High |
| UJ1.3.2 | Setting Budget Limits | Critical | Medium | Medium | High |
| UJ2.1.1 | Automatic Conversation Plan Generation | Critical | High | Medium | High |
| UJ2.2.1 | Generate All Confirmation Dialog | Critical | Low | Low | High |
| UJ2.2.2 | Batch Generation Starts with Feedback | Critical | Medium | Medium | High |
| UJ2.3.1 | Multi-Level Progress Visualization | Critical | High | Medium | High |
| UJ2.3.2 | Background Processing Persistence | Critical | High | High | High |
| UJ3.1.1 | Dashboard Overview with Sortable Table | Critical | Medium | Low | High |
| UJ3.2.1 | Persona and Emotion Filtering | Critical | Medium | Low | High |
| UJ4.1.1 | Formatted Turn-by-Turn Display | Critical | Medium | Low | High |
| UJ4.3.1 | Single Conversation Approval | Critical | Low | Low | High |
| UJ6.1.1 | Pre-Export Filtering and Selection | Critical | Low | Low | High |
| UJ6.1.2 | Format Selection and Preview | Critical | Medium | Low | High |
| UJ6.2.1 | Initiating Export and Download | Critical | Medium | Low | High |

#### High Priority (Important for Quality Experience)

| UJ ID | Element | Priority | Effort | Risk | User Impact |
|-------|---------|----------|--------|------|-------------|
| UJ1.2.2 | Emotional Arc Templates Preview | High | Low | Low | Medium |
| UJ2.1.2 | Reviewing and Adjusting Conversation Plans | High | Medium | Low | Medium |
| UJ3.1.2 | Search and Quick Filtering | High | Low | Low | High |
| UJ3.2.2 | Topic, Intent, and Tier Filtering | High | Medium | Low | High |
| UJ4.1.2 | Preview Navigation Between Conversations | High | Low | Low | High |
| UJ4.3.2 | Bulk Approval Actions | High | Medium | Low | High |
| UJ5.1.1 | Approval Progress Visibility | High | Low | Low | Medium |

#### Medium Priority (Enhances User Experience)

| UJ ID | Element | Priority | Effort | Risk | User Impact |
|-------|---------|----------|--------|------|-------------|
| UJ3.3.1 | Visual Coverage Dashboard | Medium | High | Low | Medium |
| UJ3.3.2 | Gap Recommendations and Quick Actions | Medium | Medium | Low | Medium |
| UJ4.2.1 | Quality Score Breakdown | Medium | Medium | Low | Medium |
| UJ4.2.2 | Low-Quality Conversation Flagging | Medium | Low | Low | Medium |
| UJ4.4.1 | Single Conversation Regeneration | Medium | Medium | Medium | Medium |
| UJ5.1.2 | Reviewer Activity Audit Trail | Medium | Low | Low | Low |
| UJ5.2.1 | Coverage Completeness Check | Medium | Medium | Low | Medium |
| UJ5.2.2 | Quality Distribution Validation | Medium | Low | Low | Medium |
| UJ5.3.1 | Final Cost Summary | Medium | Low | Low | Medium |
| UJ6.2.2 | Export Confirmation and Next Steps | Medium | Low | Low | Medium |

#### Low Priority (Nice to Have, Future Enhancement)

| UJ ID | Element | Priority | Effort | Risk | User Impact |
|-------|---------|----------|--------|------|-------------|
| UJ6.3.1 | Export History Tracking | Low | Low | Low | Low |
| UJ6.3.2 | Conversation Dataset Iteration | Low | Low | Low | Low |

---

## Implementation Guidance

### Suggested Development Sequence Optimized for Proof-of-Concept

**Phase 1: Foundation (Weeks 1-3)**
1. Database schema (conversations, conversation_turns, metadata)
2. Single conversation generation API endpoint
3. Basic conversation table UI
4. Simple preview panel
5. Approval buttons (approve/reject)

**Deliverable**: User can generate one conversation, view it, approve it

**Phase 2: Batch & Progress (Weeks 4-6)**
6. Batch generation API with rate limiting
7. Progress tracking (database-backed status)
8. Multi-level progress visualization UI
9. Background processing persistence
10. Filtering by persona/emotion/status

**Deliverable**: User can generate 90 conversations, monitor progress, filter results

**Phase 3: Quality & Export (Weeks 7-8)**
11. Quality validation scoring algorithm
12. Coverage analysis and visualization
13. Bulk approval actions
14. Export formatting and download
15. Cost tracking and summary

**Deliverable**: Complete workflow from setup to LoRA-ready export

### MVP vs. Enhanced Feature Delineation

**MVP (Minimum Viable Product) - Must Have:**
- Single and batch conversation generation
- Real-time progress tracking
- Conversation table with sort/filter
- Preview panel with turn-by-turn display
- Approve/reject workflow
- Quality scoring (basic: turn count, length)
- Export to JSON (OpenAI format minimum)
- Cost estimation and tracking

**Enhanced (v2) - Nice to Have:**
- Advanced coverage visualization with heatmaps
- Gap recommendations with auto-generate
- Template management UI
- Export format conversion (Anthropic, generic)
- Export history with reproducibility
- Audit trail reporting
- Version comparison side-by-side

**Future (v3+) - Long Term:**
- Real-time collaboration (multi-user approval)
- A/B testing templates with analytics
- Advanced quality scoring (semantic similarity)
- Integration with training platforms (RunPod, Replicate)
- Conversation editing and refinement UI
- Automated quality improvement suggestions

### Technical Spike Recommendations

**Spike 1: Claude API Rate Limiting Strategy (Week 1)**
- **Goal**: Determine optimal rate limiting approach
- **Tasks**: Test exponential backoff, queue-based throttling, parallel processing limits
- **Outcome**: Rate limiting configuration that maximizes throughput while respecting API limits

**Spike 2: Progress Tracking Architecture (Week 2)**
- **Goal**: Decide between polling vs. WebSocket for progress updates
- **Tasks**: Benchmark polling frequency impact on database, evaluate WebSocket complexity
- **Outcome**: Polling-based solution with 2-5 second intervals (simpler, sufficient)

**Spike 3: Database Query Performance with Filters (Week 4)**
- **Goal**: Ensure multi-dimensional filtering performs well at scale
- **Tasks**: Test queries with 1000+ conversations, analyze index usage, optimize WHERE clauses
- **Outcome**: Index strategy ensuring <500ms query time for any filter combination

**Spike 4: Export File Generation at Scale (Week 7)**
- **Goal**: Validate export performs well with large datasets (1000+ conversations)
- **Tasks**: Test JSON generation with 1000 conversations, measure memory usage, consider streaming
- **Outcome**: Server-side generation strategy handling up to 1000 conversations in <10 seconds

### Integration Points and Dependencies

**External Dependencies:**
- **Anthropic Claude API**: Core conversation generation, must be available and performant
- **Supabase Database**: Persistent storage for conversations, turns, metadata, must scale to 10,000+ conversations
- **Next.js App Router**: UI framework, server components for performance
- **Shadcn/UI Components**: Pre-built UI components for rapid development

**Internal Module Dependencies:**
- **Document Categorization (Stage 1)**: Must be complete before conversation generation
- **Chunk Extraction (Stage 2)**: Must be complete, provides 60 dimensions for conversation metadata
- **Authentication**: Reuses existing Supabase auth from Stages 1 & 2
- **API Response Logging**: Reuses existing logging infrastructure

**Data Flow Dependencies:**
```
Documents (Stage 1)
  ↓ [categories, business value]
Chunks (Stage 2)
  ↓ [60 dimensions: persona, topic, emotion, etc.]
Conversations (Stage 3)
  ↓ [auto-populated metadata]
Training Data Export
  ↓ [LoRA fine-tuning]
Personalized AI Model
```

### Development Sequencing Rationale

**Why This Order:**

1. **Database First**: Schema defines everything else, must be solid foundation
2. **Single Before Batch**: Prove core generation works before scaling
3. **Basic UI Before Advanced**: Get feedback on fundamentals before polish
4. **Progress Before Filters**: Users tolerate delays better with visibility
5. **Approval Before Export**: Quality gate prevents premature data export
6. **Export Last**: Final integration point, requires all previous stages working

**Iteration Strategy:**
- Week 1-3: Get single conversation working end-to-end (full vertical slice)
- Week 4-6: Scale to batch processing with progress (prove scalability)
- Week 7-8: Polish with filtering, quality validation, export (production-ready)

**Risk Mitigation:**
- Early spikes address highest-risk unknowns (rate limiting, progress tracking)
- Incremental delivery allows course correction
- Each phase delivers working feature users can test
- Parallel development possible (UI + backend) after Week 3

---

## Document Purpose

This user journey document serves five critical functions:

### 1. Map Complete User Experience Through Progressive Stages

**What It Does:**
- Chronicles user journey from initial login through final LoRA export
- Defines 6 major stages with clear entry/exit criteria and value delivery
- Provides turn-by-turn guidance through 3-5 hour workflow
- Establishes success criteria for each stage (e.g., "User feels confident starting")

**Why It Matters:**
- Users need clear roadmap to navigate complex workflow without getting lost
- Product team aligns on user experience vision before building
- Stakeholders understand value progression and time investment

### 2. Provide Granular Acceptance Criteria Enabling Functional Requirements Development

**What It Does:**
- Defines 50+ UJ elements with detailed GIVEN/WHEN/THEN acceptance criteria
- Specifies exact user interactions, system responses, and expected outcomes
- Documents technical notes, data requirements, error scenarios, performance targets
- Creates traceable foundation for functional requirements (UJ1.1.1 → FR3.2.4)

**Why It Matters:**
- Functional requirements inherit precise criteria from user journey (no ambiguity)
- Development tasks derive directly from acceptance criteria (clear scope)
- QA engineers write tests from GIVEN/WHEN/THEN format (testability)

### 3. Maintain User-Centric Focus While Supporting Technical Implementation

**What It Does:**
- Writes acceptance criteria from user perspective ("User sees...", "User clicks...")
- Avoids technical jargon in user-facing descriptions (smart 10th grader level)
- Includes technical notes section for implementation guidance
- Balances business value language with engineering precision

**Why It Matters:**
- Non-technical stakeholders (Sarah, Michael) understand requirements clearly
- Technical leaders (Dr. Chen) have implementation details needed
- Product managers bridge business goals with technical execution
- User empathy drives design decisions (not just technical feasibility)

### 4. Enable Clear Understanding of User Needs and Desired Outcomes

**What It Does:**
- Describes user goals, motivations, pain points for each persona
- Documents emotional states ("User feels confident", "User feels relieved")
- Explains why features matter to users (value justification)
- Shows how stages compound to deliver complete value proposition

**Why It Matters:**
- Developers understand user context behind features (builds empathy)
- Product decisions prioritize user impact over technical elegance
- User testing validates actual user needs (not assumed requirements)
- Success metrics align with user satisfaction (not just completion)

### 5. Support Progressive Development Following User Journey Sequence

**What It Does:**
- Organizes features by user journey stage (discovery → setup → generation → review → validation → export)
- Recommends development sequence matching user workflow (single → batch → bulk)
- Identifies dependencies between stages (chunks must precede conversations)
- Enables incremental delivery with working features at each milestone

**Why It Matters:**
- Developers build in logical order matching user experience
- Each development phase delivers testable user value
- Stakeholders see progress aligned with user journey milestones
- Risk reduced through incremental validation with real users

---

## User Journey Guidelines

### 1. User Experience and Value Delivery Focus

**Every element focuses on user experience:**
- Starts with user need or goal
- Describes what user experiences, not technical implementation
- Measures success from user perspective (approval rate, time saved, confidence level)
- Documents emotional outcomes ("User feels confident", "User understands clearly")

**Value delivery explicit:**
- Each stage delivers measurable user value (e.g., "90 conversations auto-planned")
- Value compounds across stages (chunks enable better conversations)
- Time investment justified by outcome (3-5 hours vs. 2-3 weeks)
- ROI clear at each milestone (approval progress, export readiness)

### 2. User Perspective Throughout Acceptance Criteria

**GIVEN/WHEN/THEN from user viewpoint:**
- GIVEN describes user context, not system state
- WHEN describes user action, not system event
- THEN describes user-visible outcome, not backend process
- AND extends user experience, not technical side-effects

**Examples:**
- ✓ Good: "THEN: User sees progress bar showing 42 of 90 complete (47%)"
- ✗ Bad: "THEN: Database updates conversation status to 'generating'"
- ✓ Good: "WHEN: User clicks 'Generate All' button"
- ✗ Bad: "WHEN: System receives POST request to /api/conversations/generate-all"

### 3. Technical Requirements Support User Experience Objectives

**Technical notes separated but aligned:**
- User acceptance criteria state WHAT user experiences
- Technical notes describe HOW to implement
- Both sections support same user outcome

**Example:**
```
User Acceptance Criteria:
- THEN: User sees real-time progress updates every 2-5 seconds

Technical Notes:
- Polling-based updates query conversations table WHERE status='generating'
- Client-side interval: 3000ms
- Database query optimized with indexes on status field
```

**Balance maintained:**
- User-facing descriptions non-technical (smart 10th grader level)
- Technical notes provide implementation guidance (engineer level)
- Both perspectives documented for complete understanding

### 4. User Terminology Preserved While Enabling Technical Implementation

**User language:**
- "Generate conversations" not "Execute API call to Claude"
- "Review and approve" not "Update database status field"
- "Export training data" not "Serialize JSON and stream response"

**Technical mapping clear:**
- User terminology maps to technical implementation in Technical Notes
- Engineers translate user actions to API endpoints, database operations
- But user stories remain in user language (testable with real users)

**Tooltips and help text:**
- Explains AI concepts simply ("Training data teaches AI to sound like you")
- Analogies to familiar concepts ("Like learning: foundation, practice, challenges")
- Progressive disclosure (simple first, details on demand)

### 5. User Journey Enables Validation Against User Needs and Satisfaction

**Testable with real users:**
- Acceptance criteria written as observable user outcomes
- Can be validated through user testing sessions (no technical knowledge required)
- Success measured by user reactions ("feels confident", "understands clearly")

**User satisfaction metrics:**
- Quantitative: time saved, approval rate, quality scores
- Qualitative: confidence level, emotional state, recommendation likelihood
- Both captured in acceptance criteria for validation

**Continuous validation:**
- Each stage independently testable (discovery complete before setup)
- User feedback informs next stage development
- Iterative refinement based on actual user experience (not assumed needs)

---

## Summary

This user journey document maps the complete experience for small business owners, content managers, and technical leaders as they generate 90-100 LoRA-ready training conversations through the Bright Run platform. By organizing the journey into 6 progressive stages—Discovery, Setup, Generation, Organization, Review, and Export—we ensure users achieve 95% time savings (3-5 hours vs. 2-3 weeks) while maintaining rigorous quality control.

**Key Differentiators:**
- **User-Centric Language**: Smart 10th grader comprehension throughout
- **Granular Acceptance Criteria**: GIVEN/WHEN/THEN format enabling precise functional requirements
- **Progressive Value Building**: Each stage compounds previous stage value
- **Non-Technical Focus**: Tooltips, analogies, visual cues for intelligent non-technical users
- **Quality Standards Alignment**: 80%+ approval rate, <$2 per conversation, sub-2-hour completion

**Strategic Impact:**
- **Immediate**: Users confidently navigate complex workflow independently
- **Short-term**: Complete training datasets enable personalized AI launch
- **Long-term**: Democratized AI access levels playing field for small businesses

This document serves as the foundation for functional requirements generation, ensuring every feature specification traces back to explicit user needs and measurable user value.

---

**Document Status:** Draft for Review  
**Next Step:** Generate functional requirements document (03-train-functional-requirements.md) using this user journey as foundation  
**Approval Required:** Product Manager, UX Lead, Technical Leader, Business Owner Representative

