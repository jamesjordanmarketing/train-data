# Chunks-Alpha to Train-Data Bridge - Functional Requirements v0.1

**Version:** 0.1
**Date:** 2025-11-13
**Status:** Draft for Review
**Scope:** Interface between Chunks-Alpha categorization module and Train-Data conversation generation engine

---

## Executive Summary

This document defines the functional requirements for the **Chunks-Train Bridge** - the critical interface layer that transforms categorized document chunks into emotionally intelligent LoRA training conversations. This bridge addresses the current gap between the document categorization workflow (chunks-alpha) and the conversation generation engine (train-data).

**Critical Finding:** The user's initial assumptions require significant refinement. The current architecture has partial bridge capabilities but lacks the structured mapping from rich categorization metadata to the complex conversation generation schema.

---

## Critical Analysis of Current State

### What Exists Today

**1. Chunks-Alpha Categorization Module**
- 3-step UI workflow for document categorization
- Outputs: Statement of Belonging (1-5 rating), Primary Category (11 options), Secondary Tags (7 dimensions)
- Status: Defined in functional requirements, implementation status unclear
- Database table: Likely requires new schema or extension of existing chunks/chunk_dimensions tables

**2. Train-Data Conversation Generation Engine**
- Robust generation system with Claude API integration
- Template system with variable resolution
- Existing chunk integration via `chunks-integration.ts`
- Database tables: conversations, conversation_turns, chunks, chunk_dimensions
- Current chunks system: 177 chunks with semantic dimensions (NOT chunks-alpha metadata)

**3. Partial Bridge (Existing)**
```typescript
// Current capabilities in lib/generation/
- chunks-integration.ts: Fetches chunks with dimensions
- dimension-parameter-mapper.ts: Maps semantic dimensions → basic parameters
- prompt-context-builder.ts: Injects chunk text into prompts
```

### What's Missing

**Critical Gaps:**

1. **Schema Mismatch**: Chunks-alpha categorization metadata (Statement of Belonging, 11 primary categories, 7 tag dimensions) is NOT present in current `chunk_dimensions` table

2. **Metadata Transformation**: No mapping from chunks-alpha categorization → conversation generation parameters:
   - How does "Statement of Belonging: 4/5" influence conversation design?
   - How does "Primary Category: Proprietary Strategies" map to persona/emotion/topic?
   - How do Secondary Tags (Authorship, Content Format, etc.) inform generation parameters?

3. **Template Selection Logic**: No system to select appropriate conversation template based on chunk category and tags

4. **Conversation Planning**: No orchestration layer that takes a categorized chunk and produces a full `GenerationParams` specification

5. **Batch Processing**: No workflow for processing multiple chunks → generating diverse conversation portfolio

---

## Validation of User's Solution Assumptions

### Assumption Analysis (Honest Assessment)

**Assumption 1:** "The chunks must serve as the input to train-data"

**Verdict:** ✅ CORRECT IN PRINCIPLE, BUT INCOMPLETE IN EXECUTION

**Analysis:**
- Chunks SHOULD be a primary input source, but they need to be CATEGORIZED chunks (with chunks-alpha metadata), not just extracted chunks
- The current chunk extraction system and chunks-alpha categorization system appear to be two different or sequential processes
- The bridge needs to connect categorized chunks, not raw chunks

**Assumption 2:** "The chunks must be converted to the proper annotations needed to provide key information to the conversation generation prompt templates"

**Verdict:** ⚠️ PARTIALLY CORRECT, BUT OVERSIMPLIFIED

**Analysis:**
- The conversation schema is VASTLY more complex than "annotations from chunks"
- Chunk data can inform SOME fields:
  - `topic` (from chunk content/summary)
  - `persona` (suggested from audience tags)
  - `tier` (influenced by Statement of Belonging rating)
  - Prompt context (chunk text injection)
- BUT chunks CANNOT provide:
  - Complete emotional context analysis (detected_emotions, emotional_indicators, behavioral_assessment)
  - Response strategy design (primary_strategy, tactical_choices, specific_techniques)
  - Response breakdown (sentence-by-sentence word choice rationales)
  - Training metadata (quality scores, learning objectives)

These are GENERATED by the AI during conversation creation, not "converted from chunks"

**Assumption 3:** "The train-data engine really has only three inputs: (a) JSON schema, (b) Prompt template, (c) Chunk-specific data"

**Verdict:** ❌ INCORRECT - SIGNIFICANT OVERSIMPLIFICATION

**Analysis:**
The train-data engine requires:

1. **Template Selection** (which template to use for this chunk?)
2. **Generation Parameters** (from current `GenerationParams` interface):
   ```typescript
   {
     templateId: string,
     persona: string,
     emotion: string,
     topic: string,
     tier: TierType,
     parameters: Record<string, any>,
     temperature?: number,
     maxTokens?: number,
     documentId?: string,
     chunkId?: string,
     createdBy?: string
   }
   ```
3. **Prompt Construction**:
   - Base template text
   - Variable substitution (persona, emotion, topic, etc.)
   - Chunk context injection (optional)
   - Conversation examples (for few-shot learning)
4. **AI Generation Request**:
   - Model selection (claude-sonnet-4-5)
   - Temperature, max tokens
   - System prompt + user prompt
5. **Post-Processing**:
   - Response parsing
   - Quality scoring
   - Database persistence

The chunks provide INPUT DATA for step #2, but the engine is much more sophisticated than "three inputs"

### Corrected Understanding

**What Actually Happens:**

```
Categorized Chunk
    ↓
Chunk Categorization Metadata (chunks-alpha output)
    ↓
[MISSING: Bridge Layer]
    ↓
Generation Parameter Specification
    ↓
Template Selection & Resolution
    ↓
Prompt Construction (with chunk context)
    ↓
Claude API Call
    ↓
Generated Conversation (with full annotation)
    ↓
Quality Scoring & Persistence
```

The Bridge Layer needs to:
1. Read categorized chunk with all metadata
2. Analyze chunk characteristics
3. Select appropriate template(s)
4. Map chunk metadata → generation parameters
5. Construct generation request(s)
6. Handle batch processing
7. Track provenance (which conversations came from which chunks)

---

## Proposed Solution Architecture

### High-Level Design

```
┌─────────────────────────────────────────────────────────────┐
│                    Chunks-Alpha Module                       │
│  (Categorization UI - outputs categorized chunks to DB)      │
└─────────────────────┬───────────────────────────────────────┘
                      │
                      ↓
┌─────────────────────────────────────────────────────────────┐
│              Chunk Categorization Database                   │
│  Tables: chunks, chunk_categorizations (NEW)                 │
└─────────────────────┬───────────────────────────────────────┘
                      │
                      ↓
┌─────────────────────────────────────────────────────────────┐
│          BRIDGE LAYER (New Implementation)                   │
│                                                               │
│  Components:                                                  │
│  1. ChunkAnalyzer - Analyzes categorized chunk               │
│  2. TemplateSelector - Selects appropriate templates         │
│  3. ParameterMapper - Maps metadata → generation params      │
│  4. ConversationPlanner - Designs conversation spec          │
│  5. BatchOrchestrator - Processes multiple chunks            │
│  6. ProvenanceTracker - Links chunks → conversations         │
└─────────────────────┬───────────────────────────────────────┘
                      │
                      ↓
┌─────────────────────────────────────────────────────────────┐
│            Train-Data Generation Engine                      │
│  (Existing: conversation-generator.ts, template-service.ts)  │
└─────────────────────┬───────────────────────────────────────┘
                      │
                      ↓
┌─────────────────────────────────────────────────────────────┐
│              Generated Conversations Database                │
│  Tables: conversations, conversation_turns                   │
└─────────────────────────────────────────────────────────────┘
```

### Key Architectural Decisions

**1. Separation of Concerns**
- Chunks-alpha: UI workflow for categorization (human-in-the-loop)
- Bridge: Automated mapping and orchestration
- Train-data: AI-powered conversation generation

**2. Flexible Mapping**
- One chunk can generate multiple conversation variations
- Categorization metadata influences but doesn't dictate conversation design
- Template library provides structural patterns

**3. Provenance Tracking**
- Every generated conversation links back to source chunk(s)
- Audit trail for data quality and iteration

**4. Batch Processing**
- Process multiple chunks in parallel
- Generate conversation portfolios from document collections
- Rate limiting and cost management

---

## Database Schema Requirements

### New Table: `chunk_categorizations`

Stores chunks-alpha categorization workflow outputs.

```sql
CREATE TABLE chunk_categorizations (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  chunk_id UUID NOT NULL REFERENCES chunks(id) ON DELETE CASCADE,

  -- Statement of Belonging (Step A)
  statement_of_belonging INTEGER NOT NULL CHECK (statement_of_belonging BETWEEN 1 AND 5),
  relationship_strength TEXT, -- Descriptive label

  -- Primary Category (Step B)
  primary_category TEXT NOT NULL,
  -- Options: complete_systems, proprietary_strategies, customer_insights,
  --          market_research, process_documentation, knowledge_base,
  --          sales_enablement, training_materials, communication_templates,
  --          project_artifacts, external_reference

  business_value_tier TEXT NOT NULL,
  -- Options: maximum, high, medium, standard

  -- Secondary Tags (Step C) - JSONB for flexibility
  authorship TEXT NOT NULL,
  -- Options: brand_company, team_member, customer, mixed_collaborative, third_party

  content_format TEXT[],
  -- Multi-select: how_to_guide, strategy_note, case_study, story_narrative,
  --                sales_page, email, transcript, presentation_slide,
  --                whitepaper, brief_summary

  disclosure_risk INTEGER NOT NULL CHECK (disclosure_risk BETWEEN 1 AND 5),
  disclosure_risk_description TEXT,

  evidence_type TEXT[],
  -- Multi-select: metrics_kpis, quotes_testimonials, before_after_results,
  --                screenshots_visuals, data_tables, external_references

  intended_use TEXT[] NOT NULL,
  -- Multi-select: marketing, sales_enablement, delivery_operations,
  --                training, investor_relations, legal_compliance

  audience_level TEXT[],
  -- Multi-select: public, lead, customer, internal, executive

  gating_level TEXT,
  -- Single-select: public, ungated_email, soft_gated, hard_gated,
  --                 internal_only, nda_only

  -- Workflow metadata
  workflow_status TEXT DEFAULT 'draft',
  -- Options: draft, in_progress, completed, approved

  categorized_by UUID REFERENCES users(id),
  categorized_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  approved_by UUID REFERENCES users(id),
  approved_at TIMESTAMP WITH TIME ZONE,

  -- Audit
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),

  UNIQUE(chunk_id) -- One categorization per chunk
);

CREATE INDEX idx_chunk_categorizations_chunk_id ON chunk_categorizations(chunk_id);
CREATE INDEX idx_chunk_categorizations_primary_category ON chunk_categorizations(primary_category);
CREATE INDEX idx_chunk_categorizations_business_value ON chunk_categorizations(business_value_tier);
CREATE INDEX idx_chunk_categorizations_status ON chunk_categorizations(workflow_status);
```

### New Table: `conversation_chunk_provenance`

Links generated conversations back to source chunks.

```sql
CREATE TABLE conversation_chunk_provenance (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  conversation_id UUID NOT NULL REFERENCES conversations(id) ON DELETE CASCADE,
  chunk_id UUID NOT NULL REFERENCES chunks(id) ON DELETE CASCADE,
  chunk_categorization_id UUID REFERENCES chunk_categorizations(id) ON DELETE SET NULL,

  -- How the chunk influenced this conversation
  influence_type TEXT NOT NULL,
  -- Options: primary_source, context_injection, topic_inspiration,
  --          persona_suggestion, example_reference

  -- Mapping details (JSONB for flexibility)
  parameter_mappings JSONB,
  -- Example: {
  --   "persona": "derived_from_audience_level",
  --   "topic": "extracted_from_chunk_summary",
  --   "tier": "based_on_statement_of_belonging"
  -- }

  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),

  UNIQUE(conversation_id, chunk_id)
);

CREATE INDEX idx_provenance_conversation ON conversation_chunk_provenance(conversation_id);
CREATE INDEX idx_provenance_chunk ON conversation_chunk_provenance(chunk_id);
```

### Modified Table: `conversations`

Add fields to track chunk-based generation.

```sql
ALTER TABLE conversations
  ADD COLUMN generation_mode TEXT DEFAULT 'manual',
  -- Options: manual, chunk_based, template_based, batch_generated

  ADD COLUMN source_chunk_id UUID REFERENCES chunks(id) ON DELETE SET NULL,
  -- Primary source chunk (if applicable)

  ADD COLUMN chunk_influence_score NUMERIC(3,2),
  -- 0.00-1.00: How much did chunk content influence the conversation?

  ADD COLUMN category_alignment TEXT;
  -- Did the conversation align with chunk's primary category?
```

---

## Functional Requirements

### FR-001: Chunk Categorization Retrieval

**User Story:** As the bridge system, I need to retrieve fully categorized chunks so I can use rich metadata for conversation generation.

**Requirements:**
- Read chunk with all categorization metadata (Statement of Belonging, Primary Category, Secondary Tags)
- Filter chunks by categorization status (approved, completed, draft)
- Filter by business value tier (maximum, high for priority processing)
- Support batch retrieval (multiple chunks at once)

**Acceptance Criteria:**
- API endpoint: `GET /api/chunks/categorized` with filters
- Returns joined data: chunk + chunk_categorization
- Supports pagination (default 20, max 100 per request)
- Includes chunk_dimensions if available (for additional context)

---

### FR-002: Template Selection Logic

**User Story:** As the bridge system, I need to select appropriate conversation templates based on chunk categorization so conversations match content characteristics.

**Template Selection Rules:**

| Primary Category | Suggested Template Types | Rationale |
|------------------|-------------------------|-----------|
| Complete Systems & Methodologies | Confusion→Clarity, Education-focused | Systems are often complex, need clarity |
| Proprietary Strategies | Confidence-building, Values-alignment | Unique content deserves confident presentation |
| Customer Insights & Case Studies | Story-based, Transformation arcs | Leverage narrative structure |
| Process Documentation | Instructional, Step-by-step | Procedural content needs structured teaching |
| Sales Enablement | Objection-handling, Persuasion arcs | Sales context drives template choice |

**Additional Selection Factors:**
- **Statement of Belonging (1-5):**
  - 4-5: High-value templates (Tier 1-2), flagship personas
  - 2-3: Standard templates (Tier 2), varied personas
  - 1: Low-value or edge case templates (Tier 3)

- **Audience Level Tags:**
  - Executive → Sophisticated personas (David-type: pragmatic, experienced)
  - Internal → Team-focused scenarios
  - Public → Educational, accessible personas (Marcus-type: learning)

- **Content Format:**
  - Case Study → Story/narrative emotional arcs
  - How-to Guide → Instructional, step-by-step
  - Email/Transcript → Conversational, natural flow

**Requirements:**
- Template selection algorithm: `TemplateSelector.selectTemplates(categorization)`
- Returns ranked list of 1-5 template suggestions with confidence scores
- Supports manual override (user can choose different template)
- Logs selection rationale for audit

**Acceptance Criteria:**
- Function returns `TemplateSelection[]` with fields: `{templateId, templateName, confidenceScore, rationale}`
- Confidence score range: 0.00-1.00
- Selection completes in <100ms for single chunk
- Can be tested independently with mock categorization data

---

### FR-003: Parameter Mapping Service

**User Story:** As the bridge system, I need to map chunk categorization metadata to conversation generation parameters so I can construct valid generation requests.

**Mapping Rules:**

**1. Persona Selection**
```typescript
interface PersonaMapping {
  // Based on Audience Level + Content Format
  audienceLevel: ['executive'] → persona: 'David-type (Pragmatic Optimist)'
  audienceLevel: ['lead', 'customer'] → persona: 'Jennifer-type (Anxious Planner)' or 'Marcus-type (Overwhelmed Avoider)'
  audienceLevel: ['internal'] → persona: 'Team member seeking guidance'

  // Influenced by Statement of Belonging
  statementOfBelonging: 4-5 → Use flagship personas (confident, experienced)
  statementOfBelonging: 1-3 → Use learning personas (uncertain, seeking help)
}
```

**2. Emotion Selection**
```typescript
interface EmotionMapping {
  // Based on Primary Category + Content Format
  primaryCategory: 'Complete Systems' + contentFormat: 'How-to Guide' → emotion: 'Confusion' (arc: Confusion→Clarity)
  primaryCategory: 'Customer Insights' + contentFormat: 'Case Study' → emotion: 'Curiosity' or 'Hope'
  primaryCategory: 'Proprietary Strategies' → emotion: 'Excitement' or 'Determination'

  // Influenced by Disclosure Risk
  disclosureRisk: 4-5 → emotions: 'Concern', 'Anxiety', 'Caution'
  disclosureRisk: 1-2 → emotions: 'Openness', 'Curiosity', 'Interest'
}
```

**3. Topic Extraction**
```typescript
interface TopicExtraction {
  // Primary: From chunk summary or section heading
  topic: chunkDimensions.chunk_summary_1s || chunk.section_heading

  // Secondary: From key terms
  subtopics: chunkDimensions.key_terms (use for conversation complexity)

  // Context: From domain tags
  domainContext: chunkDimensions.domain_tags
}
```

**4. Tier Assignment**
```typescript
interface TierMapping {
  // Based on Statement of Belonging + Business Value
  statementOfBelonging: 4-5 + businessValue: 'maximum' → tier: 'template' (Tier 1)
  statementOfBelonging: 2-3 + businessValue: 'high' → tier: 'scenario' (Tier 2)
  statementOfBelonging: 1 → tier: 'edge_case' (Tier 3)

  // Override: Complex chunks may need higher tier
  if (chunkDimensions.key_terms.length > 10) → consider tier: 'scenario'
}
```

**5. Temperature & Creativity**
```typescript
interface TemperatureMapping {
  // Based on Content Format + Intended Use
  contentFormat: ['How-to Guide'] + intendedUse: ['Training'] → temperature: 0.6 (structured)
  contentFormat: ['Story/Narrative'] + intendedUse: ['Marketing'] → temperature: 0.8 (creative)
  contentFormat: ['Case Study'] → temperature: 0.7 (balanced)
}
```

**Requirements:**
- Service: `ParameterMapper.mapCategorizationToParams(categorization, chunk, dimensions?)`
- Returns `MappedGenerationParams` with confidence scores for each field
- Supports default values when mapping is ambiguous
- Provides mapping rationale (for transparency)

**Acceptance Criteria:**
- Returns structured object with all required generation parameters
- Includes confidence scores (0.00-1.00) for each mapped field
- Mapping completes in <50ms
- Handles missing optional fields gracefully (uses defaults)
- Logs mapping decisions for audit trail

---

### FR-004: Conversation Planning Orchestrator

**User Story:** As the bridge system, I need to create a complete conversation generation plan from a categorized chunk so I can generate high-quality training conversations.

**Conversation Plan Structure:**

```typescript
interface ConversationPlan {
  chunkId: string;
  categorizationId: string;

  // Generation specifications (can be multiple variations)
  generationSpecs: GenerationSpec[];

  // Metadata
  planCreatedAt: string;
  planCreatedBy: string;
  estimatedCost: number; // USD
  estimatedDuration: number; // ms
  priority: number; // 1-10 (based on business value)
}

interface GenerationSpec {
  specId: string;
  templateId: string;
  templateName: string;

  // Mapped parameters
  persona: string;
  emotion: string;
  topic: string;
  tier: TierType;

  // Additional parameters
  parameters: {
    chunkContext: boolean; // Include chunk text in prompt?
    targetTurns: number; // 3-5 typical
    temperature: number;
    maxTokens: number;

    // Chunk-specific placeholders
    chunkSummary: string;
    chunkKeyTerms: string[];
    chunkDomain: string[];
  };

  // Provenance
  mappingConfidence: number; // 0.00-1.00
  mappingRationale: string;

  // Variations
  variationIndex: number; // 1, 2, 3 (if generating multiple)
  variationType: string; // 'primary', 'alternative_persona', 'alternative_emotion'
}
```

**Planning Logic:**

1. **Single-Spec Planning** (default):
   - One conversation per chunk
   - Best-match template + parameters
   - Suitable for initial generation

2. **Multi-Spec Planning** (variation generation):
   - Generate 3-5 conversation variations per chunk
   - Vary: persona, emotion, template
   - Same topic/content, different angles
   - Suitable for high-value chunks (Statement of Belonging: 4-5)

3. **Batch Planning**:
   - Process multiple chunks together
   - Ensure persona/emotion diversity across batch
   - Prioritize by business value tier
   - Manage rate limits and costs

**Requirements:**
- Planner: `ConversationPlanner.createPlan(categorization, options)`
- Options: `{variationCount: number, includeContext: boolean, prioritize: 'quality' | 'speed'}`
- Validates plan before returning (checks template availability, parameter completeness)
- Estimates cost and duration based on template complexity

**Acceptance Criteria:**
- Returns valid `ConversationPlan` ready for execution
- All `GenerationSpec` objects are complete and executable
- Cost estimate within 10% accuracy (based on token counts)
- Supports 1-10 variations per chunk
- Plan creation completes in <200ms for single chunk

---

### FR-005: Batch Generation Orchestrator

**User Story:** As a user, I want to process multiple categorized chunks in batch so I can generate a conversation portfolio efficiently.

**Batch Processing Workflow:**

```
1. Select chunks (filter by category, business value, status)
   ↓
2. Create conversation plans for each chunk
   ↓
3. Prioritize plans (business value, dependencies)
   ↓
4. Execute generations (parallel, rate-limited)
   ↓
5. Track progress (% complete, errors, cost)
   ↓
6. Persist conversations + provenance
   ↓
7. Generate batch report
```

**Batch Configuration:**

```typescript
interface BatchConfig {
  name: string;
  description?: string;

  // Chunk selection
  chunkFilters: {
    primaryCategory?: string[];
    businessValueTier?: string[];
    statementOfBelongingMin?: number;
    categorizationStatus?: string[];
  };

  // Generation options
  variationsPerChunk: number; // 1-10
  includeChunkContext: boolean;
  priorityMode: 'quality' | 'speed' | 'cost';

  // Rate limiting
  maxConcurrent: number; // Max parallel generations
  maxCostUsd?: number; // Stop if cost exceeds
  maxDuration?: number; // Stop if time exceeds (ms)

  // Output
  saveToPath?: string;
  notifyOnComplete?: boolean;

  createdBy: string;
}
```

**Progress Tracking:**

```typescript
interface BatchProgress {
  batchId: string;
  status: 'planning' | 'running' | 'paused' | 'completed' | 'failed';

  totalChunks: number;
  chunksProcessed: number;

  totalPlans: number;
  plansCompleted: number;
  plansFailed: number;

  totalConversations: number;
  conversationsGenerated: number;

  costUsd: number;
  durationMs: number;

  errors: BatchError[];

  startedAt: string;
  estimatedCompletionAt: string;
  completedAt?: string;
}
```

**Requirements:**
- API: `POST /api/batch/generate` with `BatchConfig`
- Returns `batchId` immediately (async processing)
- Polling endpoint: `GET /api/batch/{batchId}/progress`
- Pause/Resume: `POST /api/batch/{batchId}/pause`, `POST /api/batch/{batchId}/resume`
- Cancel: `POST /api/batch/{batchId}/cancel`
- Handles errors gracefully (retry logic, skip and continue)
- Persists batch state (survives restarts)

**Acceptance Criteria:**
- Can process 100+ chunks without failure
- Rate limiting prevents Claude API overload
- Progress updates in real-time (WebSocket or polling <5s)
- Generates provenance records for all conversations
- Batch report includes summary stats, cost breakdown, error log
- Failed generations can be retried individually

---

### FR-006: Provenance Tracking & Audit Trail

**User Story:** As a data quality manager, I need to track which conversations were generated from which chunks so I can evaluate quality and iterate on categorization.

**Provenance Requirements:**

1. **Automatic Linking**:
   - Every chunk-based conversation links to source chunk(s)
   - Record influence type (primary source, context, inspiration)
   - Store parameter mappings (how chunk influenced generation)

2. **Reverse Lookup**:
   - Given conversation, find source chunks
   - Given chunk, find all generated conversations
   - Filter by influence type, mapping confidence

3. **Quality Feedback Loop**:
   - Track conversation quality scores
   - Aggregate by chunk category, business value tier
   - Identify high-performing chunk types
   - Identify low-performing mappings (for refinement)

**Requirements:**
- Service: `ProvenanceTracker.linkConversationToChunk(conversationId, chunkId, influence, mappings)`
- Query: `ProvenanceTracker.getConversationsForChunk(chunkId)`
- Query: `ProvenanceTracker.getChunksForConversation(conversationId)`
- Analytics: `ProvenanceTracker.analyzeQualityByCategory()`
- Export: `ProvenanceTracker.exportProvenanceReport(batchId)`

**Acceptance Criteria:**
- All chunk-based conversations have provenance records
- Queries return results in <100ms for single lookup
- Analytics aggregations complete in <2s for 1000+ conversations
- Provenance export includes full audit trail (CSV or JSON)

---

### FR-007: Manual Override & Refinement

**User Story:** As a conversation designer, I want to manually override automatic parameter mappings when the bridge makes incorrect suggestions.

**Override Capabilities:**

1. **Pre-Generation Override**:
   - Review generated conversation plan
   - Modify template selection
   - Adjust mapped parameters (persona, emotion, topic)
   - Add custom parameters
   - Save as "reviewed plan" before execution

2. **Post-Generation Refinement**:
   - Review generated conversation
   - Provide quality feedback
   - Flag for regeneration with different parameters
   - Update mapping rules based on feedback

3. **Mapping Rule Customization**:
   - Define custom mapping rules per organization
   - Override default persona/emotion mappings
   - Set preferred templates for specific categories
   - Configure variation strategies

**Requirements:**
- UI: Conversation plan review interface
- API: `PUT /api/conversation-plans/{planId}` with override fields
- API: `POST /api/mapping-rules` to create custom rules
- Versioning: Track plan versions (original vs overridden)
- Feedback: `POST /api/conversations/{id}/feedback` with quality assessment

**Acceptance Criteria:**
- User can modify any field in conversation plan before execution
- Override reasons are logged (for learning)
- Custom mapping rules persist across sessions
- Feedback influences future automated mappings (ML improvement)
- Override interface is intuitive (form-based, not JSON editing)

---

## API Interface Specifications

### POST /api/bridge/analyze-chunk

Analyze a categorized chunk and suggest conversation parameters.

**Request:**
```json
{
  "chunkId": "uuid",
  "options": {
    "includeTemplateAnalysis": true,
    "includeParameterSuggestions": true,
    "variationCount": 3
  }
}
```

**Response:**
```json
{
  "chunkId": "uuid",
  "categorization": {
    "statementOfBelonging": 4,
    "primaryCategory": "proprietary_strategies",
    "businessValueTier": "high"
  },
  "analysis": {
    "topic": "AI-powered financial planning methodologies",
    "complexity": "high",
    "suggestedPersonas": [
      {"persona": "Jennifer-type", "confidence": 0.85, "rationale": "..."},
      {"persona": "David-type", "confidence": 0.72, "rationale": "..."}
    ],
    "suggestedEmotions": [
      {"emotion": "Curiosity", "confidence": 0.88},
      {"emotion": "Cautious optimism", "confidence": 0.75}
    ],
    "suggestedTemplates": [
      {
        "templateId": "uuid",
        "templateName": "Confusion→Clarity",
        "confidence": 0.91,
        "rationale": "Complex proprietary strategies benefit from clarity-building arc"
      }
    ]
  }
}
```

---

### POST /api/bridge/create-plan

Create a conversation generation plan from categorized chunk.

**Request:**
```json
{
  "chunkId": "uuid",
  "options": {
    "variationCount": 3,
    "includeChunkContext": true,
    "priority": "quality",
    "overrides": {
      "persona": "David-type",
      "emotion": "Determined"
    }
  }
}
```

**Response:**
```json
{
  "planId": "uuid",
  "chunkId": "uuid",
  "generationSpecs": [
    {
      "specId": "uuid-1",
      "templateId": "uuid",
      "templateName": "Confusion→Clarity",
      "persona": "David-type",
      "emotion": "Determined",
      "topic": "AI-powered financial planning",
      "tier": "template",
      "parameters": {
        "chunkContext": true,
        "targetTurns": 4,
        "temperature": 0.7,
        "maxTokens": 2048
      },
      "mappingConfidence": 0.89,
      "variationType": "primary"
    }
  ],
  "estimatedCost": 0.45,
  "estimatedDuration": 8500,
  "priority": 8
}
```

---

### POST /api/bridge/execute-plan

Execute a conversation generation plan.

**Request:**
```json
{
  "planId": "uuid",
  "options": {
    "executeAll": true,
    "trackProvenance": true
  }
}
```

**Response:**
```json
{
  "executionId": "uuid",
  "status": "running",
  "totalSpecs": 3,
  "completedSpecs": 0,
  "conversations": [],
  "estimatedCompletionAt": "2025-11-13T14:35:00Z"
}
```

---

### POST /api/bridge/batch-generate

Batch process multiple chunks.

**Request:**
```json
{
  "batchConfig": {
    "name": "Q4 Proprietary Content Batch",
    "chunkFilters": {
      "primaryCategory": ["proprietary_strategies", "complete_systems"],
      "businessValueTier": ["maximum", "high"],
      "statementOfBelongingMin": 3
    },
    "variationsPerChunk": 2,
    "includeChunkContext": true,
    "priorityMode": "quality",
    "maxConcurrent": 5,
    "maxCostUsd": 100.00,
    "createdBy": "user-uuid"
  }
}
```

**Response:**
```json
{
  "batchId": "uuid",
  "status": "planning",
  "totalChunksSelected": 47,
  "estimatedPlans": 94,
  "estimatedCost": 85.30,
  "estimatedDuration": 1200000,
  "message": "Batch planning in progress. Poll /api/batch/{batchId}/progress for updates."
}
```

---

## Implementation Considerations

### Phase 1: Foundation (Weeks 1-2)

**Deliverables:**
1. Database schema implementation (`chunk_categorizations`, `conversation_chunk_provenance`)
2. Basic chunk categorization API (if not already implemented)
3. ChunkAnalyzer service (reads categorized chunks)
4. Basic ParameterMapper (simple rule-based mappings)

**Testing:**
- Unit tests for ParameterMapper rules
- Integration test: Categorized chunk → Mapped parameters
- Database migration and rollback scripts

---

### Phase 2: Core Bridge (Weeks 3-4)

**Deliverables:**
1. TemplateSelector service (template selection logic)
2. ConversationPlanner service (creates generation plans)
3. ProvenanceTracker service (linking and audit)
4. API endpoints: `/api/bridge/analyze-chunk`, `/api/bridge/create-plan`

**Testing:**
- End-to-end test: Chunk → Plan → Execute (1 conversation)
- Provenance verification test
- Template selection accuracy test (manual review)

---

### Phase 3: Batch & Orchestration (Weeks 5-6)

**Deliverables:**
1. BatchOrchestrator service (parallel processing, rate limiting)
2. Progress tracking system (database + WebSocket)
3. API endpoints: `/api/bridge/batch-generate`, `/api/batch/{id}/progress`
4. Error handling and retry logic

**Testing:**
- Load test: 100 chunks → 200 conversations
- Rate limiting verification (doesn't exceed Claude API limits)
- Error recovery test (simulated API failures)
- Cost estimation accuracy validation

---

### Phase 4: UI & Refinement (Weeks 7-8)

**Deliverables:**
1. Conversation plan review UI
2. Override interface
3. Batch monitoring dashboard
4. Provenance visualization
5. Analytics dashboard (quality by category)

**Testing:**
- User acceptance testing (conversation designers)
- Override workflow testing
- Analytics accuracy verification

---

### Phase 5: Optimization & Learning (Weeks 9-10)

**Deliverables:**
1. Mapping rule refinement (based on quality feedback)
2. Custom mapping rules configuration UI
3. Automated quality feedback loop
4. Documentation and training materials

**Testing:**
- A/B test: Default mappings vs refined mappings
- Quality improvement measurement (before/after)

---

## Success Metrics

### Technical Metrics

1. **Mapping Accuracy**: 80%+ of automated parameter mappings rated "good" or better by human reviewers
2. **Generation Success Rate**: 95%+ of planned conversations generate without errors
3. **Provenance Completeness**: 100% of chunk-based conversations have provenance records
4. **Performance**: Batch processing of 100 chunks completes in <2 hours
5. **Cost Efficiency**: Average cost per conversation <$1.00 USD

### Business Metrics

1. **Conversation Quality**: 80%+ of chunk-based conversations score 4+ quality rating (human review)
2. **High-Value Coverage**: 100% of high-value chunks (Statement of Belonging 4-5) generate conversations
3. **Diversity**: Conversation portfolio covers 10+ persona types, 15+ emotion types
4. **Iteration Speed**: Time from chunk categorization to first conversation <24 hours
5. **User Satisfaction**: 80%+ of conversation designers rate bridge system as "helpful" or better

---

## Appendix A: Mapping Examples

### Example 1: High-Value Proprietary Content

**Input Chunk:**
- **Primary Category**: Proprietary Strategies & Approaches
- **Statement of Belonging**: 5
- **Business Value**: Maximum
- **Authorship**: Brand/Company
- **Audience Level**: Executive, Customer
- **Content Format**: Strategy Note, Case Study

**Automated Mapping:**
- **Template**: Values Alignment, Transformation Arc
- **Persona**: David-type (Pragmatic Optimist) - executive level
- **Emotion**: Curious, Determined
- **Topic**: [Extracted from chunk summary]
- **Tier**: Template (Tier 1)
- **Temperature**: 0.75 (creative but grounded)
- **Include Context**: Yes
- **Target Turns**: 4-5 (complex content deserves depth)

**Rationale**: High-value proprietary strategy targeting executives should showcase transformation, use sophisticated persona, maintain strategic tone.

---

### Example 2: Process Documentation

**Input Chunk:**
- **Primary Category**: Process Documentation & Workflows
- **Statement of Belonging**: 3
- **Business Value**: Standard
- **Authorship**: Team Member
- **Audience Level**: Internal
- **Content Format**: How-to Guide

**Automated Mapping:**
- **Template**: Confusion→Clarity, Instructional
- **Persona**: Marcus-type (Overwhelmed Avoider) - learning mode
- **Emotion**: Confusion, Uncertainty
- **Topic**: [Process name from chunk]
- **Tier**: Scenario (Tier 2)
- **Temperature**: 0.6 (structured, clear)
- **Include Context**: Yes
- **Target Turns**: 3-4 (step-by-step education)

**Rationale**: Process documentation serves internal learners, should focus on clarity and step-by-step guidance, practical tone.

---

### Example 3: Customer Case Study

**Input Chunk:**
- **Primary Category**: Customer Insights & Case Studies
- **Statement of Belonging**: 4
- **Business Value**: High
- **Authorship**: Customer, Brand/Company (Mixed)
- **Audience Level**: Lead, Customer
- **Content Format**: Case Study, Story/Narrative
- **Evidence Type**: Before/After Results, Quotes/Testimonials

**Automated Mapping:**
- **Template**: Transformation Arc, Hope→Empowerment
- **Persona**: Jennifer-type (Anxious Planner) seeking validation
- **Emotion**: Hope, Cautious optimism
- **Topic**: [Customer success story theme]
- **Tier**: Template (Tier 1)
- **Temperature**: 0.8 (narrative, emotional)
- **Include Context**: Yes (story details)
- **Target Turns**: 4-5 (build emotional arc)

**Rationale**: Customer case studies should leverage narrative structure, build hope, use evidence to support transformation, connect emotionally with prospects.

---

## Appendix B: Database ERD

```
┌─────────────────────────┐
│       documents         │
│─────────────────────────│
│ id (PK)                 │
│ title                   │
│ ... (existing fields)   │
└───────────┬─────────────┘
            │
            │ 1:N
            ↓
┌─────────────────────────┐
│        chunks           │
│─────────────────────────│
│ id (PK)                 │
│ document_id (FK)        │
│ chunk_text              │
│ ... (existing fields)   │
└───────────┬─────────────┘
            │
            ├─────────────────────────┐
            │ 1:1                     │ 1:N
            ↓                         ↓
┌────────────────────────────┐  ┌──────────────────────────────┐
│  chunk_categorizations     │  │  conversation_chunk_         │
│  (NEW)                     │  │  provenance (NEW)            │
│────────────────────────────│  │──────────────────────────────│
│ id (PK)                    │  │ id (PK)                      │
│ chunk_id (FK) UNIQUE       │  │ conversation_id (FK)         │
│ statement_of_belonging     │  │ chunk_id (FK)                │
│ primary_category           │  │ chunk_categorization_id (FK) │
│ business_value_tier        │  │ influence_type               │
│ authorship                 │  │ parameter_mappings (JSONB)   │
│ content_format[]           │  └──────────────┬───────────────┘
│ disclosure_risk            │                 │
│ evidence_type[]            │                 │ N:1
│ intended_use[]             │                 ↓
│ audience_level[]           │  ┌────────────────────────────┐
│ gating_level               │  │     conversations          │
│ workflow_status            │  │────────────────────────────│
│ categorized_by (FK)        │  │ id (PK)                    │
│ categorized_at             │  │ conversation_id            │
└────────────────────────────┘  │ generation_mode (NEW)      │
                                │ source_chunk_id (FK) (NEW) │
                                │ chunk_influence_score (NEW)│
                                │ ... (existing fields)      │
                                └────────────────────────────┘
```

---

## Appendix C: Glossary

**Bridge Layer**: The interface system that transforms categorized chunks into conversation generation specifications.

**Chunks-Alpha**: The document categorization module UI workflow that produces rich metadata (Statement of Belonging, Primary Categories, Secondary Tags).

**Conversation Plan**: A structured specification that defines how to generate one or more conversations from a categorized chunk.

**Generation Spec**: A single conversation generation configuration within a plan (template, parameters, options).

**Mapping Confidence**: A 0.00-1.00 score indicating how confident the bridge is in its parameter mappings.

**Provenance**: The audit trail linking generated conversations back to their source chunks.

**Statement of Belonging**: A 1-5 rating indicating how closely a document represents the organization's unique voice and expertise.

**Template Selection**: The process of choosing appropriate conversation templates based on chunk categorization.

**Tier**: Classification of conversation complexity (Tier 1: Template-driven, Tier 2: Scenario-based, Tier 3: Edge cases).

**Variation**: Multiple different conversations generated from the same source chunk by varying persona, emotion, or template.

---

## Document Status

**Version:** 0.1
**Status:** Draft for Review
**Next Steps:**
1. Review with stakeholders
2. Validate technical feasibility
3. Refine mapping rules based on sample chunks
4. Prioritize FR implementation order
5. Create detailed implementation tickets

**Feedback Requested:**
- Are the mapping rules sensible for your content types?
- Is the database schema complete?
- Which features are highest priority for Phase 1?
- Are there edge cases not covered?

---

*This document represents an honest, critical analysis of the current state and proposed solution. The user's initial assumptions were well-intentioned but oversimplified. This refined approach provides a structured path forward with clear requirements, realistic implementation phases, and measurable success criteria.*


# Appendix 1

Model Comments and Quick Info:

Deep Research Analysis Complete

  I've completed a comprehensive analysis of your system and created the functional
  requirements document at:
  C:\Users\james\Master\BrightHub\brun\train-data\pmc\product\03-chunks-train-bridge-func    
  tional-requirements_v.01.md

  Critical Findings (Honest Assessment)

  Your Assumptions - Validation:

  1. "Chunks must serve as input to train-data" → ✅ CORRECT IN PRINCIPLE, BUT INCOMPLETE    
    - You're conflating two different chunk systems:
        - Chunks-alpha UI: The categorization workflow (Statement of Belonging, 11
  categories, 7 tag dimensions)
      - Current chunks system: The existing extraction/dimensions in the codebase (177       
  chunks)
    - These need to be connected or may be different stages
  2. "Chunks must be converted to annotations" → ⚠️ OVERSIMPLIFIED
    - Chunk metadata provides INPUT DATA (topic, persona hints, context)
    - But the massive annotation structure (emotional_context, response_strategy,
  response_breakdown) is GENERATED by Claude during conversation creation, not "converted    
   from chunks"
    - This is a critical misunderstanding
  3. "Three inputs to train-data" → ❌ INCORRECT - Too simplistic
    - The actual engine is far more complex than 3 inputs
    - It's a multi-stage pipeline: Template Selection → Parameter Mapping → Prompt
  Construction → AI Generation → Post-Processing

  What's Actually Missing: THE BRIDGE LAYER

  The bridge you need has 6 core components:

  1. ChunkAnalyzer - Reads categorized chunks with rich metadata
  2. TemplateSelector - Picks appropriate conversation templates based on chunk
  characteristics
  3. ParameterMapper - Maps chunk metadata → generation parameters (persona, emotion,        
  topic, tier)
  4. ConversationPlanner - Creates complete generation specifications
  5. BatchOrchestrator - Processes multiple chunks with rate limiting
  6. ProvenanceTracker - Links conversations back to source chunks

  Key Database Gap

  You need a new table: chunk_categorizations to store the chunks-alpha UI output
  (Statement of Belonging, Primary Categories, Secondary Tags). This data doesn't exist      
  in the current chunk_dimensions table.

  Implementation Path Forward

  The document outlines a 5-phase, 10-week implementation plan with clear deliverables,      
  API specifications, and mapping rules.

  Priority 1 (Weeks 1-2): Database schema + basic parameter mapping
  Priority 2 (Weeks 3-4): Core bridge services + single-conversation flowPriority 3 
  (Weeks 5-6): Batch processing + orchestration

  Most Important Insight

  The "seed 10" conversations were created using massive manual prompt engineering (see      
  the 1493-line prompt specification). The bridge system you're building should AUTOMATE     
  much of that process by using chunk categorization to drive template selection and
  parameter generation - but it won't eliminate the need for Claude to generate the
  detailed emotional annotations.

  Review the document and let me know which sections need clarification or if you
  disagree with any of my analysis.