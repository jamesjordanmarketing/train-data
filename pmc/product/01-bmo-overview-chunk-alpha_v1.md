REQUIREMENTS
I want you to carefully train yourself on the codebase here: `C:\Users\james\Master\BrightHub\BRun\chunks-alpha\src` and understand everything that it does.

Right now it is the first module required in the LoRA document training pipeline. See `C:\Users\james\Master\BrightHub\BRun\chunks-alpha\pmc\product\01-bmo-overview.md` to understand the whole pipeline.

Read this:

`C:\Users\james\Master\BrightHub\BRun\chunks-alpha\pmc\product\01-bmo-overview-categ-module_v1.md` to understand the spec that was used to build this current codebase.

Read this:
`C:\Users\james\Master\BrightHub\BRun\chunks-alpha\pmc\system\plans\context-carries\context-carry-info-10-03-25-508pm.md` to understand the most recent actions taken on this build.

Next we are testing varying ways of organizing and collecting the document chunk dimensions. This is the first unstructured requirements doc to build this data testing environment.

I have decided that we need to get our arms around the entire dimension ontologies by building a generation engine that makes it easy for me to see and evaluate all the chunk dimensions so I can define the best way to improve the quality of the dimension generation.

The best way I can think to do this is to:

1. See all the data in one spreadsheet. The current world of data about a chunk is here:
`C:\Users\james\Master\BrightHub\BRun\chunks-alpha\system\chunks-alpha-data\document-metadata-dictionary.csv`. Use these dimensions as the baseline data dimensions that we must build for each chunk.

2. Have meta dimensions that can help us understand & sort the data. Meta Dimensions examples: to
a. dimension criticalness to a successful LoRA interpretation (1-10)
b. dimension category type (a column that has a shared category value that organizes the dimensions into relevant categories i.e.)
- IP_Sensitivity
- PII_Flag
both could be part of a data privacy category with a column value of "privacy"

c. A column with the required AI context engineering type required to create the row's data values . I.e.

Doc_ID = mechanically generated (no AI needed to create)
Author = generated by the previous step (no AI needed to create)
Audience = In depth analysis of the entire document & the specific chunk needed by AI (help me think of a way to say this better & help me think of the different types of AI analysis needed by this column)
suggested context engineering category: this dimension needs advanced context engineering in order to have a confidently accurate and precise value (help me think of a way to say this better))

d. A column with rating 1-10 of how confident the system is in generating the value purely from AI analysis and already provided document categorization

e. A numeric column dedicated to the cost to generate the default values. I guess this is just tokens times model per token cost.

f. Include two confidence level columns that evaluates the LLM's confidence in its generated answer's precision and accuracy, i.e. Precision-Confidence & Accuracy-Confidence. They have a rating 1-10 based purely on the AI analysis and already provided document categorization

g. can you think of any more very helpful meta dimensions? Don't go wild. You can create a maximum of 5 dimensions if I have not thought of them and they are very high priority for this process.

3. This lets me test varying documents by submitting them to the module which generates chunk values which I can then have a human evaluate.

4. Output all the chunk values to a simple web page spreadsheet. We want to be able to see ALL of this data in a web page that looks very much like a spreadsheet. It should have some of the same functions of a spreadsheet too, like sort by column value, filter by column value, header field.


SPECIFICATION
So what we need to do is write a specification that will:

A. Start with this codebase as the initial iteration of this new module.

B. Update this codebase in place so it becomes the new codebase with the added functionality

C. Audit the Supabase tables. I have recently upgraded it and now to my understanding all belonging choice, category, and tag selections at the document level are being recorded in the Supabase database. We must be able to use these as input for the chunk default tagging and labeling that this specification will implement. Audit the current database and make sure you can access and use all data that was input in the Categ-module 3 panel data collection submissions. You will use this to both split the document into chunks and also to default surmise the tags and labels at the chunk level.

D. The workflow must be: after the category module is completed and approved by the user the document will then have a new button on the current document Dashboard (i.e. https://chunks-alpha.vercel.app/dashboard).
Right now each document has the button: `Start Categorization`
after the Categorization is finished a new button called "Chunks" will appear below `Start Categorization`. Clicking on Chunks for the first time will auto gn
on the

D. Auto Extract Chunks
The first step after the user presses the document into the following chunk types, which are prioritized for modern, label-rich workflows where models can auto-annotate a lot out of the box:


- Chapter (Structural/Section) Chunks
Why now: Provide the document’s macro-structure for hierarchical prompting, summarization, retrieval, and evaluation. Chapter/section chunks anchor Instructional, CER, and Example chunks to a stable outline, enabling scoped prompts (e.g., “only reason within Stage 2”) and controllable context windows for modern LLMs.
How to detect: Top-level headings and section markers such as “Chapter/Section/Stage/Part,” numbered outlines (e.g., 1./1.1), Table-of-Contents anchors, PDF bookmarks, style cues (H1/H2), page-break patterns, and consistent heading typography. Prefer canonical hierarchy from source (bookmarks/TOC) over heuristic line parsing when both exist.
Key columns to fill:
Section_Heading, Page_Start, Page_End, Char_Start, Char_End, Token_Count, Overlap_Tokens, Chunk_Handle, Chunk_Summary_1s, Key_Terms, Audience, Intent, overage_Tag, Novelty_Tag, Primary_Category (inherit from doc unless overridden), Chunk_Type = Chapter_Sequential, Embedding_ID, Vector_Checksum, Include_In_Training_YN, Data_Split_Train_Dev_Test, Augmentation_Notes (these fields almost already exist in the current defined dimensions here: `C:\Users\james\Master\BrightHub\BRun\chunks-alpha\system\chunks-alpha-data\document-metadata-dictionary.csv`, for those that don't add them after making sure there isn't already a field with a different name that serves the same purpose).
How Many: As many needed so all logical chapters of the document are present


- Instructional Unit (Procedure/Task) Chunks
Why now: Best alignment with instruction-tuning and agentic tool use. Maps directly to input→steps→expected output.
How to detect: Look for verbs, imperative mood, numbered/bulleted steps, “How to…”, “Procedure”, “Checklist”, etc.
Key columns to fill: Task_Name, Preconditions, Inputs, Steps_JSON, Expected_Output, Warnings_Failure_Modes, plus Prompt_Candidate/Target_Answer for trainable pairs (these fields almost already exist in the current defined dimensions here: `C:\Users\james\Master\BrightHub\BRun\chunks-alpha\system\chunks-alpha-data\document-metadata-dictionary.csv`, for those that don't add them after making sure there isn't already a field with a different name that serves the same purpose).
How Many: As many as present in the document to a maximum of 5


- Claim–Evidence–Reasoning (CER) Chunks
Why now: Current models benefit from structured factuality and rationale; CER improves truthful, sourced generations.
How to detect: Assertions with “because/therefore,” statistics, references, citations, graphs/tables nearby.
Key columns: Claim, Evidence_Snippets, Reasoning_Sketch, Citations, Factual_Confidence_0_1, and Compliance_Flags where needed (these fields may already exist in the current defined dimensions here: `C:\Users\james\Master\BrightHub\BRun\chunks-alpha\system\chunks-alpha-data\document-metadata-dictionary.csv`, for those that don't add them after making sure there isn't already a field with a different name that serves the same purpose).
How Many: As many as present in the document to a maximum of 10

- Example / Scenario / Dialogue Chunks
Why now: Great for style transfer, brand voice, and applied problem-solution patterns (e.g., customer conversations).
How to detect: Case studies, “for example,” transcripts, Q&As, role-plays.
Key columns: Scenario_Type, Problem_Context, Solution_Action, Outcome_Metrics, Style_Notes, and optionally derive Prompt_Candidate/Target_Answer (these fields may already exist in the current defined dimensions here: `C:\Users\james\Master\BrightHub\BRun\chunks-alpha\system\chunks-alpha-data\document-metadata-dictionary.csv`, for those that don't add them after making sure there isn't already a field with a different name that serves the same purpose).
How Many: As many as present in the document to a maximum of 5


E. After all the default chunks have been extracted they must be labelled and tagged for each field in `C:\Users\james\Master\BrightHub\BRun\chunks-alpha\system\chunks-alpha-data\document-metadata-dictionary.csv` format.
It must gather and collect already generated tags (like chunk #, Document associated
For the fields that need generative thinking it will do that too.

E. Uses the OpenAI api standard interface (allow for choice of the api endpoint wanted to generate the default dimensions. Start with having Claude Sonnet 4.5 as the default and only api endpoint for this iteration). We don't need a settings section or pages to set the api endpoint. I will manually place the endpoint and key in the code)

F. Build very simple prompt contexts as a default for the robust context engineering we will do per prompt in the next iteration. We will investigate that separately. This iteration should though have the functionality to use multiple types of context engineering. Use very simple context. I.E. "This is an industry vertical research context prompt. Search these verticals [VERTICAL_INDUSTRY_PROMPT]". Then build the code to submit several different types of prompts based on the value of the AI context engineering type field described above. Use a Contract-styled format (JSON + JSON Schema) as the primary engineered context prompt format. Validate input and submit the JSON to the LLM API.


G. Build the api calls necessary to extract "first pass" chunk dimension values all chunks that need AI context engineering (an example of a task that uses AI but does not need context engineering is asking the AI to extract a simple text string from a document. Those must be differentiated from those that need context engineering or "generative" answers). This involves the submission of the prompt format to the model correctly.

H. All chunk, metadata, referential data, factual data, tags, labels, and generated dimensions content should be saved to well structured, normalized, human readable named and organized Supabase tables.

I. After all the default chunks have been tagged and labelled they must be displayed on a "[NAME_OF_DOCUMENT] - Chunk Dashboard" that is very much like this codebase: `C:\Users\james\Master\BrightHub\BRun\chunks-alpha\chunks-alpha-dashboard\src`. You can use this exact UI and update it to match our required functionality. Remember it is a VITE application so cannot be used as is. It is better to use it to understand the look and behavior of the chunks panels.


`C:\Users\james\Master\BrightHub\BRun\chunks-alpha\src`. Under "things we know" list just three AI generated dimensions for its accuracy confidence level). Under "things we don't know" list just three generated dimensions that we did not generate or that have an accuracy confidence level of less than 75%
Clicking on one of them will take you to the spreadsheet page of all the associated data for that chunk.

J. The results page for each associated data for the final outputs of a chunk run shall:
i. The data for the run will be displayed to the results page within the module. It should contain as much of the information within one standard screen size as possible. I.E. Very much like a simple spreadsheet view with little space between rows or columns. It should be sortable by many columns (you pick the ones that should logically be sortable)

ii. Each results page must have a unique page name (e.g. chunk name + datetimestamp) so that multiple pages can be viewed at a time

iii. I must be able to regenerate any chunk I want from the document's chunk dashboard. We do not need to create manual inputs at the per dimension level for this iteration. But I will test different engineered context input prompts by manually updating the prompt on the back end. It should generate a unique id for each page so it can be looked up and selected to see again.

iv. I must be able to look up and compare runs. I.e. a "run selector" each run can have chunk name & timestamp in the run name. The chunk selection should be on the "per chunk level". I.e. If I am looking at the Chunk A page, I should only see historical look ups for Chunk A.

v. The results page does NOT include the ability to tag or label any of these dimensions by a human. This iteration is dedicated to creating the default dimensions for a chunk. So it must generate the chunk dimension values using only the information collected from the current category module step.

How the agent must Execute this Product Build

I have found that creating too many sequential tasks to develop a product creates situations where the current prompt redoes the work of the prior prompt and leads to a fragmented codebase. I have found it is preferential to have one large prompt that can work on as much as possible within the same context.

So come up with a strategy and specification wherein:
1. We do as much prework as possible that will not affect the final product build. Things that cannot lose context such as upgrading the database, adding the LLM API credentials,  or other required but not "in module code" work.
a. Segment the build prompts into as few prompts as possible (while still maintaining a high probability of a quality outcome).
b. For each individual prompt keep it as module as possible. Meaning that the prompt should attempt to "finish" a feature, organically related of elements, sub-module, full pages, etc., so that the next prompt does not have to enhance the previous prompt, but instead can start work on an entirely new feature, organically related group of elements, sub-module, full pages, etc.
c. Clearly identify action steps that the human guide must execute them in the build process and make sure they do not add unnecessary context tokens to the build prompts.


Current Active Goal
You must take this unstructured specification and convert it into detailed task based build directives. These directives must be in the form of directions to the human who is building this with you (me) and prompts that I will be submitting to the 200k context window Claude 4.5 Sonnet Thinking LLM.
Make sure that each prompt is clearly and obviously marked so I can cut and paste the prompt into the LLM easily


Current Active Task
Before you build the new detailed task based build directives read this prompt carefully. Examine the codebases, tables, linked files, and all details carefully, and then think carefully and determine if you have questions that will help increase the quality of this build.

